{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8장. 딥러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/WegraLee/deep-learning-from-scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 코드의 내용은 Deep Learning from Scratch를 참고했음을 밝힙니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 확장(p265)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 확장(data augmentation)은 입력 이미지(훈련 이미지)를 알고리즘을 동원해 '인위적'으로 확장합니다. 아래 그림과 같이 입력 이미지를 회전하거나 세로로 이동하는 등 미세한 변화를 주어 이미지의 개수를 늘리는 것이죠. 이는 데이터가 몇 개 없을 때 특히 효과적인 수단입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 데이터 확장의 예<br>\n",
    "![image.png](https://i.imgur.com/MzMxsnM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 확장은 위의 그림과 같은 변형 외에도 다양한 방법으로 이미지를 확장할 수 있습니다. 예를 들어 이미지 일부를 잘라내는 crop이나 좌우를 뒤집는 flip(flip은 이미지의 대칭성을 고려하지 않아도 되는 경우에만 쓸 수 있습니다) 등이 있겠죠. 일반적인 이미지에는 밝기 등의 외형 변화나 확대/축소 등의 스케일 변화도 효과적입니다. 어쨌든 데이터 확장을 동원해 훈련 이미지의 개수를 늘릴 수 있다면 딥러닝의 인식 수준을 개선할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 층을 깊게 하는 이유(p265)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. 신경망의 매개변수 수가 줄어든다는 것입니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "층을 깊게 한 신경망은 깊지 않은 경우보다 적은 매개변수로 같은 (혹은 그 이상) 수준의 표현력을 달성할 수 있습니다. 합성곱 연산에서의 필터 크기에 주목해 생각해 보면 쉽게 이해될 겁니다. 예를 하나 볼까요? 아래 그림은 5x5 필터로 구성된 합성곱 계층입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 5x5 합성곱 연산의 예<br>\n",
    "![image.png](https://i.imgur.com/dvnWCJr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기에서 주목할 점은 출력 데이터의 각 노드가 입력 데이터의 어느 영역으로부터 계산되었느냐는 것입니다. 당연하지만 위 그림의 예에서는 각각의 출력 노드는 입력 데이터의 5x5 크기 영역에서 계산됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이어서 아래 그림처럼 3x3 합성곱 연산을 2회 반복하는 경우를 생각해봅시다. 이 경우 출력 노드 하나는 중간 데이터의 3x3 영역에서 계산됩니다. 그럼 중간 데이터의 3x3 영역은 그 전 입력 데이터의 어느 영역에서 계산될까요? 아래 그림을 잘 보면 5x5 크기의 영역에서 계산되어 나오는 것을 알 수 있지요. 즉, 아래 그림의 출력 데이터는 입력 데이터의 5x5 영역을 '보고' 계산하게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3x3의 합성곱 계층을 2회 반복한 예<br>\n",
    "![image.png](https://i.imgur.com/36Kwt9S.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5x5의 합성곱 연산 1회는 3x3의 합성곱 연산을 2회 수행하여 대체할 수 있습니다. 게다가 전자의 매개변수 수가 25개(5x5)인 반면, 후자는 총 18개(2x3x3)이며, 매개변수 수는 층을 반복할수록 적어집니다. 그리고 그 개수의 차이는 층이 깊어질수록 커집니다. 예를 들어 3x3의 합성곱 연산을 3회 반복하면 매개변수는 모두 27개가 되지만, 같은 크기의 영역을 1회의 합성곱 연산으로 '보기' 위해서는 7x7 크기의 필터, 즉 매개변수 49개가 필요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "작은 필터를 겹쳐 신경망을 깊게 할 때의 장점은 매개변수 수를 줄여 넓은 수용 영역(receptive field)을 소화할 수 있다는 데 있습니다(수용 영역은 뉴런에 변화를 일으키는 국소적인 공간 영역입니다). 게다가 층을 거듭하면서 ReLU 등의 활성화 함수를 합성곱 계층 사이에 끼움으로써 신경망의 표현력이 개선됩니다. 이는 활성화 함수가 신경망에 '비선형' 힘을 가하고, 비선형 함수가 겹치면서 더 복잡한 것도 표현할 수 있게 되기 때문입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 학습의 효율성도 층을 깊게 하는 것의 이점입니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "층을 깊게 함으로써 학습 데이터의 양을 줄여 학습을 고속으로 수행할 수 있다는 뜻입니다. 이를 (직감적으로) 이해하려면 7장의 'CNN 시각화하기'에서의 설명을 생각하면 좋을 것입니다. 7장에서 CNN의 합성곱 계층이 정보를 계층적으로 추출하고 있음을 설명했습니다. 앞단의 합성곱 계층에서는 에지 등의 단순한 패턴에 뉴런이 반응하고 층이 깊어지면서 텍스처와 사물의 일부와 같이 점차 더 복잡한 것에 반응한다고 설명했죠."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런 네트워크 계층 구조를 기억해두고, '개'를 인식하는 문제를 생각해봅시다. 이 문제를 얕은 신경망에서 해결하려면 합성곱 계층은 개의 특징 대부분을 한 번에 '이해'해야 합니다. 견종도 다양하고 어느 각도에서 찍은 사진이냐 따라 완전히 다르게 보일 수 있습니다. 그래서 개의 특징을 이해하려면 변화가 풍부하고 많은 학습 데이터가 필요하고, 결과적으로 학습 시간이 오래 걸립니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나 신경망을 깊게 하면 학습해야 할 문제를 계층적으로 분해할 수 있습니다. 각 층이 학습해야 할 문제를 더 단순한 문제로 대체할 수 있다는 것이죠. 예를 들어 처음 층은 에지 학습에 전념하여 적은 학습 데이터로 효율적으로 학습할 수 있습니다. 개가 등장하는 이미지보다 에지를 포함한 이미지는 많고, 에지의 패턴은 개라는 패턴보다 구조가 훨씬 간단하기 때문이죠."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또, 층을 깊게 하면 정보를 계층적으로 전달할 수 있다는 점도 중요합니다. 예를 들어 에지를 추출한 층의 다음 층은 에지 정보를 쓸 수 있고, 더 고도의 패턴을 효과적으로 학습하리라 기대할 수 있습니다. 즉, 층을 깊이 함으로써 각 층이 학습해야 할 문제를 '풀기 쉬운 단순한 문제'로 분해할 수 있어 효율적으로 학습하리라 기대할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG(p270)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG는 합성곱 계층과 풀링 계층으로 구성되는 '기본적'인 CNN입니다. 다만, 아래 그림과 같이 비중있는 층(합성곱 계층, 완전연결 계층)을 모두 16층(혹은 19층)으로 심화한 게 특징입니다(층의 깊이에 따라서 'VGG16'과 'VGG19'로 구분하기도 합니다)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* VGG<br>\n",
    "![image.png](https://i.imgur.com/F20Drt9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG에서 주목할 점은 3x3의 작은 필터를 사용한 합성곱 계층을 연속으로 거친다는 것입니다. 그림에서 보듯 합성곱 계층을 2~4회 연속으로 풀링 계층을 두어 크기를 절반으로 처리를 반복합니다. 그리고 마지막에서는 완전연결 계층을 통과시켜 결과를 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GoogLeNet(p271)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GoogLeNet의 구성은 아래와 같습니다. 그림의 사각형이 합성곱 계층과 풀링 계층 등의 계층을 나타냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GoogLeNet(p271)<br>\n",
    "![image.png](https://i.imgur.com/ipe3jD7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그림을 보면 구성이 매우 복잡해 보이는데, 기본적으로는 지금까지 보아온 CNN과 다르지 않습니다. 단, GoogLeNet은 세로 방향 깊이뿐 아니라 가로 방향도 깊다는 점이 특징입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GoogLeNet에는 가로 방향에 '폭'이 있습니다. 이를 인셉션 구조라 하며, 그 기반 구조는 아래 그림과 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GoogLeNet의 인셉션 구조<br>\n",
    "![image.png](https://i.imgur.com/DrLkdDY.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인셉션 구조는 위의 그림과 같이 크기가 다른 필터(와 풀링)를 여러 개 적용하여 그 결과를 결합합니다. 이 인셉션 구조를 하나의 빌딩 블록(구성요소)으로 사용하는 것이 GoogLeNet의 특징인 것이죠. 또 GoogLeNet에서는 1x1 크기의 필터를 사용한 합성곱 계층을 많은 곳에서 사용합니다. 이 1x1의 합성곱 연산은 채널 쪽으로 크기를 줄이는 것으로, 매개변수 제거와 고속 처리에 기여합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet(p272)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet(Residual Network)은 마이크로소프트의 팀이 개발한 네트워크입니다. 그 특징은 지금까지보다 층을 더 깊게 할 수 있는 특별한 '장치'에 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 층을 깊게 하는 것이 성능 향상에 중요하다는 건 알고 있었습니다. 그러나 딥러닝의 학습에서는 층이 지나치게 깊으면 학습이 잘 되지 않고, 오히려 성능이 떨어지는 경우도 많습니다. ResNet에서는 그런 문제를 해결하기 위해서 스킵 연결(skip connection)을 도입합니다. 이 구조가 층의 깊이에 비례해 성능을 향상시킬 수 있게 한 핵심입니다(물론 층을 깊게 하는 데는 여전히 한계가 있습니다)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스킵 연결이란 아래 그림과 같이 입력 데이터를 합성곱 계층을 건너뛰어 출력에 바로 더하는 구조를 말합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ResNet의 구성요소 : 'weight layer'는 합성곱 계층을 말한다.<br>\n",
    "![image.png](https://i.imgur.com/QOMjrHL.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 그림에서는 입력 x를 연속한 두 합성곱 계층을 건너뛰어 출력에 바로 연결합니다. 이 단축 경로가 없었다면 두 합성곱 계층의 출력이 F(x)가 되나, 스킵 연결로 인해 F(x) + x가 되는 게 핵심입니다. 스킵 연결은 층이 깊어져도 학습을 효율적으로 할 수 있도록 해주는데, 이는 역전파 때 스킵 연결이 신호 감쇠를 막아주기 때문입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스킵 연결은 입력 데이터를 '그대로' 흘리는 것으로, 역전파 때도 상류의 기울기를 그대로 하류로 보냅니다. 여기에서의 핵심은 상류의 기울기에 아무런 수정도 가하지 않고 '그대로' 흘린다는 것이죠. 그래서 스킵 연결로 기울기가 작아지거나 지나치게 커질 걱정 없이 앞 층에 '의미 있는 기울기'가 전해지리라 기대할 수 있습니다. 층을 깊게 할수록 기울기가 작아지는 소실 문제를 이 스킵 연결이 줄여주는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet은 먼저 설명한 VGG 신경망을 기반으로 스킵 연결을 도입하여 층을 깊게 했습니다. 그 결과는 아래 그림처럼 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ResNet : 블록이 3x3인 합성곱 계층에 대응. 층을 건너뛰는 스킵 연결이 특징이다.<br>\n",
    "![image.png](https://i.imgur.com/F6eAYSW.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지넷이 제공하는 거대한 데이터셋으로 학습한 가중치 값들은 실제 제품에 활용해도 효과적이고, 또 많이들 그렇게 이용하고 있습니다. 이를 전이 학습(transfer learning)이라고 해서, 학습된 가중치(혹은 그 일부)를 다른 신경망에 복사한 다음, 그 상태로 재학습을 수행합니다. 예를 들어 VGG와 구성이 같은 신경망을 준비하고, 미리 학습된 가중치를 초깃값으로 설정한 후, 새로운 데이터셋을 대상으로 재학습(fine tuning)을 수행합니다. 전이 학습은 보유한 데이터셋이 적을 때 특히 유용한 방법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정리(p289)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 수많은 문제에서 신경망을 더 깊게 하여 성능을 개선할 수 있다.\n",
    "* 이미지 인숙 기술 대회인 ILSVRC에서는 최근 딥러닝 기반 기법이 상위권을 독점하고 있으며, 그 깊이도 깊어지는 추세다.\n",
    "* 유명한 신경망으로는 VGG, GoogLeNet, ResNet이 있다.\n",
    "* GPU와 분산 학습, 비트 정밀도 감소 등으로 딥러닝을 고속화할 수 있다.\n",
    "* 딥러닝(신경마)은 사물 인식뿐 아니라 사물 검출과 분할에도 이용할 수 있다.\n",
    "* 딥러닝의 응용 분야로는 사진의 캡션 생성, 이미지 생성, 강화학습 등이 있다. 최근에는 자율 주행에도 딥러닝을 접목하고 있어 기대된다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
