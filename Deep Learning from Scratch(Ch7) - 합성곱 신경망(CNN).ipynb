{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7장. 합성곱 신경망(CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/WegraLee/deep-learning-from-scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 코드의 내용은 Deep Learning from Scratch를 참고했음을 밝힙니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전체 구조(p227)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 합성곱 계층(convolutional layer) + 풀링 계층(pooling layer)\n",
    "* 완전연결(fully-connected, 전결합) : 인접하는 계층의 모든 뉴런과 결합\n",
    "* Affine 계층 : 완전히 연결된 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 완전연결 계층(Affine 계층)으로 이뤄진 네트워크의 예<br>\n",
    "![image.png](https://i.imgur.com/28Q4Bnq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 그림과 같이 완전연결 신경망은 Affine 계층 뒤에 활성화 함수를 갖는 ReLU 계층(혹은 Sigmoid 계층)이 이어집니다. 이 그림에서는 Affine-ReLU 조합이 4개가 쌓였고, 마지막 5번째 층은 Affine 계층에 이어 소프트맥스 계층에서  최종 결과(확률)를 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CNN으로 이뤄진 네트워크의 예 : 합성곱 계층과 풀링 계층이 새로 추가(회색)<br>\n",
    "![image.png](https://i.imgur.com/oUptKaL.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 그림과 같이 CNN에서는 새로운 '합성곱 계층(Conv)'과 '풀링 계층(Pooling)'이 추가됩니다. CNN의 계층은 'Conv-ReLU-(Pooling)' 흐름으로 연결됩니다(풀링 계층은 생략하기도 합니다). 지금까지의 'Affine-ReLU' 연결이 'Conv-ReLU-(Pooling)'으로 바뀌었다고 생각할 수 있겠죠.<br>\n",
    "위의 그림의 CNN에서 주목할 또 다른 점은 출력에 가까운 층에서는 지금까지의 'Affine-ReLU' 구성을 사용할 수 있다는 것입니다. 또, 마지막 출력 계층에서는 'Affine-Softmax' 조합을 그대로 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 완전연결 계층의 문제점(p229)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 본 완전연결 신경망에서는 완전연결 계층(Affine 계층)을 사용했습니다. 완전연결 계층에서는 인접하는 계층의 뉴런이 모두 연결되고 출력의 수는 임의로 정할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "완전연결 계층의 문제점은 무엇일까요? 바로 '데이터의 형상이 무시'된다는 사실입니다. 입력 데이터가 이미지인 경우를 예로 들면, 이미지는 통상 세로/가로/채널(색상)로 구성된 3차원 데이터입니다. 그러나 완전연결 계층에 입력할 때는 3차원 데이터를 평평한 1차원 데이터로 평탄화해줘야 합니다. 사실 지금까지의 MNIST 데이터셋을 사용한 사례에서는 형상이 (1,28,28)인 이미지(1채널, 세로 28픽셀, 가로 28픽셀)를 1줄로 세운 784개의 데이터를 첫 Affine 계층에 입력했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지는 3차원 형상이며, 이 형상에는 소중한 공간적 정보가 담겨 있죠. 예를 들어 공간적으로 가까운 픽셀은 값이 비슷하거나, RGB의 각 채널은 서로 밀접하게 관련되어 있거나, 거리가 먼 픽셀끼리는 별 연관이 없는 등, 3차원 속에서 의미를 갖는 본질적인 패턴이 숨어 있을 것입니다. 그러나 완전연결 계층은 형상을 무시하고 모든 입력 데이터를 동등한 뉴런(같은 차원의 뉴런)으로 취급하여 형상에 담긴 정보를 살릴 수 없습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한편, 합성곱 계층은 형상을 유지합니다. 이미지도 3차원 데이터로 입력받으며, 마찬가지로 다음 계층에도 3차원 데이터로 전달합니다. 그래서 CNN에서는 이미지처럼 형상을 가진 데이터를 제대로 이해할 (가능성이 있는) 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN에서는 합성곱 계층의 입출력 데이터를 특징 맵(feature map)이라고도 합니다. 합성곱 계층의 입력 데이터를 입력 특징 맵(input feature map), 출력 데이터를 출력 특징 맵(output feature map)이라고 하는 식이죠."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 합성곱 연산(p230)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "합성곱 연산은 필터의 윈도우(window)를 일정 간격으로 이동해가며 입력 데이터에 적용합니다. 여기에서 말하는 윈도우는 아래 그림의 회색 3x3 부분을 가리킵니다. 이 그림에서 보듯 입력과 필터에서 대응하는 원소끼리 곱한 후 그 총합을 구합니다(이 계산을 단일 곱셈-누산(fused multiply-add,FMA)이라 합니다). 그리고 그 결과를 출력의 해당 장소에 저장합니다. 이 과정을 모든 장소에서 수행하면 합성곱 연산의 출력이 완성됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 합성곱 연산의 계산 순서<br>\n",
    "![image.png](https://i.imgur.com/pypW5oW.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자, 완전연결 신경망에는 가중치 매개변수와 편향이 존재하는데, CNN에서는 필터의 매개변수가 그동안의 '가중치'에 해당합니다. 그리고 CNN에도 편향이 존재합니다. 위의 그림은 필터를 적용하는 단계까지만 보여준 것이고, 편향까지 포함하면 아래 그림과 같은 흐름이 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 합성곱 연산의 편향 : 필터를 적용할 원소에 고정값(편향)을 더한다.<br>\n",
    "![image.png](https://i.imgur.com/WpBkcwJ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 그림과 같이 편향은 필터를 적용한 후의 데이터에 더해집니다. 그리고 편향은 항상 하나(1x1)만 존재합니다. 그 하나의 값을 필터를 적용한 모든 원소에 더하는 것이죠."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 패딩(p232)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "합성곱 연산을 수행하기 전에 입력 데이터 주변을 특정 값(예컨대 0)으로 채우기도 합니다. 이를 패딩(padding)이라 하며, 합성곱 연산에서 자주 이용하는 기법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 합성곱 연산의 패딩 처리 : 입력 데이터 주위에 0을 채운다(패딩은 점선으로 표시했으며 그 안의 값 '0'은 생략했다).<br>\n",
    "![image.png](https://i.imgur.com/GtA90VS.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "패딩은 주로 출력 크기를 조정할 목적으로 사용합니다. 예를 들어 (4,4) 입력 데이터에 (3,3) 필터를 적용하면 출력은 (2,2)가 되어, 입력보다 2만큼 줄어듭니다. 이는 합성곱 연산을 몇 번이나 되풀이하는 심층 신경망에서 문제가 될 수 있습니다. 합성곱 연산을 거칠 때마다 크기가 작아지면 어느 시점에서는 출력 크기가 1이 되어버리겠죠. 더 이상은 합성곱 연산을 적용할 수 없다는 뜻입니다. 이러한 사태를 막기 위해 패딩을 사용합니다. 앞의 예에서는 패딩의 폭을 1로 설정하니 (4,4) 입력에 대한 출력이 같은 크기인 (4,4)로 유지되었습니다. 한 마디로 입력 데이터의 공간적 크기를 고정한 채로 다음 계층에 전달할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 스트라이드(p233)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 스트라이드가 2인 합성곱 연산<br>\n",
    "![image.png](https://i.imgur.com/C6AcvXB.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 그림에서는 크기가 (7,7)인 입력 데이터에 스트라이드를 2로 설정한 필터를 적용합니다. 이처럼 스트라이드는 필터를 적용하는 간격을 지정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력 크기를 (H,W), 필터 크기를 (FH, FW), 출력 크기를(OH, OW), 패딩을 P, 스트라이드를 S라 하면, 출력 크기는 다음 식으로 계산합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://i.imgur.com/ak9vkUD.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3차원 데이터의 합성곱 연산(p235)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3차원 데이터 합성곱 연산의 계산 순서<br>\n",
    "![image.png](https://i.imgur.com/sDZYsFh.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3차원의 합성곱 연산에서 주의할 점은 입력 데이터의 채널 수와 필터의 채널 수가 같아야 한다는 것입니다. 이 예에서는 모두 3개로 일치합니다. 한편, 필터 자체의 크기는 원하는 값으로 설정할 수 있습니다(단, 모든 채널의 필터가 같은 크기여야 합니다). 이 예에서는 (3,3)이지만, 원한다면 (2,2)나 (1,1) 또는 (5,5) 등으로 설정해도 되는 것이죠. 다시 말하지만, 필터의 채널 수는 입력 데이터의 채널 수와 같도록(이 예에서는 3) 설정해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 블록으로 생각하기(p237)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3차원의 합성곱 연산은 데이터와 필터를 직육면체 블록이라고 생각하면 쉽습니다. 또, 3차원 데이터를 다차원 배열로 나타낼 때는 (채널(channel), 높이(height), 너비(width)) 순서로 쓰겠습니다. 예를 들어 채널 수 C, 높이 H, 너비 W인 데이터의 형상은 (C, H, W)로 씁니다. 필터도 같은 순서로 씁니다. 예를 들어 채널 수 C, 필터 높이 FH(Filter Height), 필터 너비 FW(Filter Width)의 경우 (C, FH, FW)로 씁니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 합성곱 연산을 직육면체 블록으로 생각한다. 블록의 형상에 유의할 것!<br>\n",
    "![image.png](https://i.imgur.com/xGh1UC6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자, 이 예에서 출력 데이터는 한 장의 특징 맵입니다. 한 장의 특징 맵을 다른 말로 하면 채널이 1개인 특징 맵이죠. 그럼 합성곱 연산의 출력으로 다수의 채널을 내보내려면 어떻게 해야 할까요? 그 답은 필터(가중치)를 다수 사용하는 것입니다. 그림으로는 아래처럼 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 여러 필터를 사용한 합성곱 연산의 예<br>\n",
    "![image.png](https://i.imgur.com/XE1vnhz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 그림과 같이 필터를 FN개 적용하면 출력 맵도 FN개가 생성됩니다. 그리고 그 FN개의 맵을 모으면 형상이 (FN, OH, OW)인 블록이 완성됩니다. 이 완성된 블록을 다음 계층으로 넘기겠다는 것이 CNN의 처리 흐름입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이상에서 보듯 합성곱 연산에서는 필터의 수도 고려해야 합니다. 그런 이유로 필터의 가중치 데이터는 4차원 데이터이며 (출력 채널 수, 입력 채널 수, 높이, 너비) 순으로 씁니다. 예를 들어 채널 수 3, 크기 5x5인 필터가 20개 있다면 (20,3,5,5)로 씁니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자, 합성곱 연산에도 (완전연결 계층과 마찬가지로) 편향이 쓰입니다. 아래 그림은 편향을 더한 모습입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://i.imgur.com/mN6GGfm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 그림에서 보듯 편향은 채널 하나에 값 하나씩으로 구성됩니다. 이 예에서는 편향의 형상은 (FN,1,1)이고 ,필터의 출력 결과의 형상은 (FN, OH, OW)입니다. 이 두 블록을 더하면 편향의 각 값이 필터의 출력인 (FN, OH, OW) 블록의 대응 채널의 원소 모두에 더해집니다. 참고로, 형상이 다른 블록의 덧셈은 넘파이의 브로드캐스트 기능으로 쉽게 구현할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 배치 처리(p239)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 처리에서는 입력 데이터를 한 덩어리로 묶어 배치로 처리했습니다. 완전연결 신경망을 구현하면서는 이 방식을 지원하여 처리 효율을 높이고, 미니배치 방식의 학습도 지원하도록 했습니다.<br>\n",
    "합성곱 연산도 마찬가지로 배치 처리를 지원하고자 합니다. 그래서 각 계층을 흐르는 데이터의 차원을 하나 늘려 4차원 데이터로 저장합니다. 구체적으로는 데이터를 (데이터 수, 채널 수, 높이, 너비) 순으로 저장합니다. 데이터가 N개일 때 위의 그림을 배치 처리한다면 데이터 형태가 아래 그림처럼 되는 것이죠."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 합성곱 연산의 처리 흐름(배치 처리)<br>\n",
    "![image.png](https://i.imgur.com/WLtCDlQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치 처리 시의 데이터 흐름을 나타낸 위의 그림을 보면 각 데이터의 선두에 배치용 차원을 추가했습니다. 이처럼 데이터는 4차원 형상을 가진 채 각 계층을 타고 흐릅니다. 여기에서 주의할 점으로는 신경망에 4차원 데이터가 하나 흐를 때마다 데이터 N개에 대한 합성곱 연산이 이뤄진다는 것입니다. 즉, N회 분의 처리를 한 번에 수행하는 것이죠."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀링 계층(p240)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "풀링은 세로/가로 방향의 공간을 줄이는 연산입니다. 예를 들어 아래 그림과 같이 2x2 영역을 원소 하나로 집약하여 공간 크기를 줄입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 최대 풀링의 처리 순서<br>\n",
    "![image.png](https://i.imgur.com/FbcDJDP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 그림은 2x2 최대 풀링(max pooling, 맥스 풀링)을 스트라이드 2로 처리하는 순서입니다. 최대 풀링은 최댓값(max)을 구하는 연산으로, '2x2'는 대상 영역의 크기를 뜻합니다. 즉 2x2 최대 풀링은 그림과 같이 2x2 크기의 영역에서 가장 큰 원소 하나를 꺼냅니다. 또, 스트라이드는 이 예에서는 2로 설정했으므로  2x2 윈도우가 원소 2칸 간격으로 이동합니다. 참고로, 풀링의 윈도우 크기와 스트라이드는 같은 값으로 설정하는 것이 보통입니다. 예를 들어 윈도우가 3x3이면 스트라이드는 3으로, 윈도우가 4x4이면 스트라이드를 4로 설정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "풀링은 최대 풀링 외에도 평균 풀링(average pooling) 등이 있습니다. 최대 풀링은 대상 영역에서 최댓값을 취하는 연산인 반면, 평균 풀링은 대상 영역의 평균을 계산합니다. 이미지 인식 분야에서는 주로 최대 풀링을 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "풀링 계층의 특징\n",
    "* 학습해야 할 매개변수가 없다.<br>\n",
    "풀링 계층은 합성곱 계층과 달리 학습해야 할 매개변수가 없습니다. 풀링은 대상 영역에서 최댓값이나 평균을 취하는 명확한 처리이므로 특별히 학습할 것이 없습니다.\n",
    "* 채널 수가 변하지 않는다.<br>\n",
    "풀링 연산은 입력 데이터의 채널 수 그대로 출력 데이터로 내보냅니다. 아래 그림처럼 채널마다 독립적으로 계산하기 때문입니다.\n",
    "* 입력의 변화에 영향을 적게 받는다(강건하다).<br>\n",
    "입력 데이터가 조금 변해도 풀링의 결과는 잘 변하지 않습니다. 예를 들어 아래 그림은 입력 데이터의 차이(데이터가 오른쪽으로 1칸씩 이동)를 풀링이 흡수해 사라지게 하는 모습을 보여줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 풀링은 채널 수를 바꾸지 않는다.<br>\n",
    "![image.png](https://i.imgur.com/u5hpbW1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 입력 데이터가 가로로 1원소만큼 어긋나도 출력은 같다(데이터에 따라서는 다를 수도 있다).<br>\n",
    "![image.png](https://i.imgur.com/Tke90g1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1, 28, 28)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.random.rand(10, 1, 28, 28) # 무작위로 데이터 생성\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28)\n",
      "(1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# 첫 번째 데이터에 접근하려면 단순히 x[0]이라고 씁니다.\n",
    "# 마찬가지로 두 번째 데이터는 x[1] 위치에 있습니다.\n",
    "\n",
    "print(x[0].shape) # (1,28,28)\n",
    "print(x[1].shape) # (1,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.83922377, 0.09492244, 0.03310394, 0.02141913, 0.05490539,\n",
       "        0.99822166, 0.62079021, 0.2878635 , 0.71387633, 0.39362699,\n",
       "        0.6617209 , 0.67285569, 0.37721904, 0.62911814, 0.14783186,\n",
       "        0.29272972, 0.15452631, 0.9191135 , 0.02902563, 0.01928973,\n",
       "        0.86317634, 0.11836224, 0.96256884, 0.35227865, 0.64963188,\n",
       "        0.4004125 , 0.78354242, 0.1568943 ],\n",
       "       [0.53986518, 0.15721865, 0.49387677, 0.18071801, 0.97275608,\n",
       "        0.11364591, 0.35015653, 0.00805281, 0.293566  , 0.07502261,\n",
       "        0.677656  , 0.89253819, 0.86903777, 0.69933312, 0.1908317 ,\n",
       "        0.9963933 , 0.18622978, 0.62321687, 0.92191282, 0.38034917,\n",
       "        0.90406593, 0.83899332, 0.18416932, 0.95841108, 0.11736501,\n",
       "        0.8300413 , 0.88012831, 0.77665321],\n",
       "       [0.3010306 , 0.75138449, 0.1667221 , 0.8765602 , 0.96748336,\n",
       "        0.96698561, 0.75019938, 0.89383147, 0.1770091 , 0.93178432,\n",
       "        0.45854058, 0.8327606 , 0.63580009, 0.03954435, 0.38177702,\n",
       "        0.21019266, 0.82554435, 0.40268957, 0.68031599, 0.61481208,\n",
       "        0.53410209, 0.66982741, 0.03612909, 0.83940577, 0.15702022,\n",
       "        0.43252016, 0.52611727, 0.65461794],\n",
       "       [0.05508752, 0.53132187, 0.04649821, 0.1284562 , 0.36940129,\n",
       "        0.44708887, 0.33871715, 0.05258841, 0.00820959, 0.85266389,\n",
       "        0.58797391, 0.27672203, 0.14404684, 0.79355911, 0.76848264,\n",
       "        0.51365753, 0.97784949, 0.09567778, 0.85013893, 0.79332201,\n",
       "        0.94564377, 0.45088316, 0.18092374, 0.76725781, 0.67089266,\n",
       "        0.86651384, 0.83321792, 0.5881584 ],\n",
       "       [0.13108819, 0.75640053, 0.86668599, 0.18162586, 0.1787687 ,\n",
       "        0.44967458, 0.44503374, 0.54927359, 0.90671952, 0.08040718,\n",
       "        0.24777288, 0.17633661, 0.71721104, 0.96678561, 0.98591123,\n",
       "        0.00327165, 0.61970932, 0.08964326, 0.668543  , 0.26402079,\n",
       "        0.51673424, 0.25371234, 0.67403079, 0.83745269, 0.07557122,\n",
       "        0.79828346, 0.41684167, 0.1591057 ],\n",
       "       [0.9916933 , 0.59681882, 0.79048713, 0.51237094, 0.01191814,\n",
       "        0.2444157 , 0.10270863, 0.11472621, 0.69869114, 0.35670953,\n",
       "        0.02065921, 0.54590941, 0.63187829, 0.75417684, 0.07703076,\n",
       "        0.96544982, 0.48909665, 0.8809611 , 0.3899001 , 0.62752609,\n",
       "        0.64799159, 0.90944999, 0.24272255, 0.48508485, 0.02444937,\n",
       "        0.2633574 , 0.06737202, 0.26610648],\n",
       "       [0.48152076, 0.55367811, 0.10926179, 0.61349925, 0.03411641,\n",
       "        0.60673777, 0.02210815, 0.18775265, 0.4543479 , 0.91553264,\n",
       "        0.00637623, 0.93456595, 0.0894305 , 0.54594154, 0.64705544,\n",
       "        0.62179409, 0.59247229, 0.51957093, 0.50922272, 0.14310952,\n",
       "        0.24253482, 0.16791373, 0.62798559, 0.47962217, 0.86438711,\n",
       "        0.25837732, 0.89845767, 0.40927965],\n",
       "       [0.85060018, 0.5589044 , 0.49001253, 0.16949458, 0.83985673,\n",
       "        0.36893725, 0.6761221 , 0.42214317, 0.10461388, 0.39910423,\n",
       "        0.22554628, 0.43777096, 0.19016875, 0.95492481, 0.8524798 ,\n",
       "        0.02653593, 0.40237582, 0.05251828, 0.61511395, 0.88854259,\n",
       "        0.94121363, 0.21419346, 0.30998204, 0.22942744, 0.81608471,\n",
       "        0.35572126, 0.56744057, 0.21183458],\n",
       "       [0.23964739, 0.79689386, 0.78599073, 0.40418813, 0.4190519 ,\n",
       "        0.78899089, 0.16685581, 0.807871  , 0.39170287, 0.54819231,\n",
       "        0.39029836, 0.82612868, 0.31133081, 0.80938241, 0.72078529,\n",
       "        0.9778165 , 0.15141494, 0.25894705, 0.175601  , 0.85463211,\n",
       "        0.70366116, 0.63933666, 0.68244467, 0.68950242, 0.26481168,\n",
       "        0.43862095, 0.5884127 , 0.685804  ],\n",
       "       [0.84236779, 0.90958666, 0.01785875, 0.05176741, 0.73000382,\n",
       "        0.40884199, 0.23574071, 0.01702501, 0.37350778, 0.42510056,\n",
       "        0.25477676, 0.38560181, 0.05521957, 0.53037075, 0.6958742 ,\n",
       "        0.40491074, 0.40185197, 0.08603948, 0.41692251, 0.8099948 ,\n",
       "        0.04869944, 0.26647365, 0.75011314, 0.69785392, 0.49917354,\n",
       "        0.63407618, 0.27159545, 0.29166273],\n",
       "       [0.41273399, 0.33590202, 0.22033155, 0.60001828, 0.01827932,\n",
       "        0.09885399, 0.72118164, 0.56180679, 0.03137791, 0.58764406,\n",
       "        0.22759128, 0.57245059, 0.53067515, 0.50354551, 0.69000233,\n",
       "        0.34300263, 0.19532128, 0.28653256, 0.40557545, 0.61918759,\n",
       "        0.19293202, 0.27998064, 0.11230651, 0.08301611, 0.77498085,\n",
       "        0.71341953, 0.8928635 , 0.8195772 ],\n",
       "       [0.25246924, 0.07429294, 0.15331169, 0.69527944, 0.22604232,\n",
       "        0.56442874, 0.8158984 , 0.33776057, 0.61283562, 0.58297319,\n",
       "        0.00605837, 0.45251353, 0.55182515, 0.76885573, 0.37522392,\n",
       "        0.92836494, 0.47388323, 0.46771174, 0.58309655, 0.00476852,\n",
       "        0.18820755, 0.8734869 , 0.25012205, 0.77214455, 0.90150189,\n",
       "        0.40993879, 0.84915152, 0.6057369 ],\n",
       "       [0.27470705, 0.9520046 , 0.31115745, 0.83081131, 0.5654985 ,\n",
       "        0.58811057, 0.2674662 , 0.70334245, 0.32770937, 0.03223914,\n",
       "        0.25152839, 0.10176847, 0.4760497 , 0.95079241, 0.61990717,\n",
       "        0.94134587, 0.08572359, 0.07088615, 0.61490057, 0.00146403,\n",
       "        0.14631189, 0.76806466, 0.59042294, 0.16750034, 0.38561057,\n",
       "        0.14585995, 0.79163934, 0.52863665],\n",
       "       [0.71570936, 0.98677729, 0.73997269, 0.02410232, 0.29300224,\n",
       "        0.04007137, 0.33421741, 0.01381749, 0.97483475, 0.45379939,\n",
       "        0.89297734, 0.71840491, 0.70171509, 0.03216392, 0.24081892,\n",
       "        0.96595527, 0.42482275, 0.82248337, 0.86398235, 0.40366081,\n",
       "        0.55028054, 0.82732607, 0.16621671, 0.10361343, 0.28676917,\n",
       "        0.35936634, 0.27261243, 0.29359057],\n",
       "       [0.36768487, 0.83752245, 0.00401099, 0.4023983 , 0.90173981,\n",
       "        0.84181231, 0.14629984, 0.64324106, 0.00795275, 0.5134269 ,\n",
       "        0.51740579, 0.85940195, 0.56134568, 0.69612793, 0.82968876,\n",
       "        0.26091992, 0.00103039, 0.76360263, 0.79173046, 0.37993468,\n",
       "        0.83262498, 0.01287421, 0.21289301, 0.11604351, 0.58190304,\n",
       "        0.89849181, 0.91209865, 0.98731722],\n",
       "       [0.36061064, 0.85306187, 0.4976012 , 0.2678515 , 0.65853213,\n",
       "        0.34242346, 0.97765042, 0.0702325 , 0.8870195 , 0.75706389,\n",
       "        0.86913863, 0.20080321, 0.60291673, 0.97795536, 0.20750166,\n",
       "        0.08234511, 0.28413123, 0.88579429, 0.20502535, 0.1973309 ,\n",
       "        0.77651608, 0.74938483, 0.24465498, 0.57162841, 0.18330546,\n",
       "        0.51661271, 0.09978362, 0.44961908],\n",
       "       [0.92558425, 0.59141056, 0.57744743, 0.05738995, 0.31961466,\n",
       "        0.92949715, 0.31015627, 0.91280661, 0.41039056, 0.21184355,\n",
       "        0.74920814, 0.97304001, 0.61067138, 0.18202006, 0.99167765,\n",
       "        0.62676385, 0.93193914, 0.31658902, 0.15430326, 0.97214131,\n",
       "        0.179054  , 0.60479925, 0.46634785, 0.59132996, 0.06807079,\n",
       "        0.17202719, 0.88803277, 0.05234947],\n",
       "       [0.48472245, 0.19104102, 0.66649589, 0.90964268, 0.66224269,\n",
       "        0.84329487, 0.63321129, 0.7135554 , 0.43591245, 0.41998869,\n",
       "        0.87006298, 0.82543359, 0.8600086 , 0.7658209 , 0.16468563,\n",
       "        0.2736414 , 0.59974597, 0.36949802, 0.448341  , 0.05933692,\n",
       "        0.26633897, 0.47397421, 0.57204855, 0.4749972 , 0.25970541,\n",
       "        0.95662456, 0.41807125, 0.32263551],\n",
       "       [0.86396679, 0.05341784, 0.5235675 , 0.34305539, 0.93333752,\n",
       "        0.89151201, 0.41119687, 0.09185524, 0.01146595, 0.57034865,\n",
       "        0.29945884, 0.00398336, 0.93999037, 0.62674577, 0.95878352,\n",
       "        0.36476468, 0.43192974, 0.559542  , 0.93352454, 0.60823792,\n",
       "        0.60207955, 0.10225839, 0.87154402, 0.20539584, 0.58735707,\n",
       "        0.17202427, 0.89145266, 0.59195046],\n",
       "       [0.72948617, 0.53522464, 0.90208823, 0.4967747 , 0.33496851,\n",
       "        0.2885973 , 0.94068836, 0.83883392, 0.4824582 , 0.5944428 ,\n",
       "        0.74485564, 0.36006309, 0.90643307, 0.52121699, 0.57359077,\n",
       "        0.67982431, 0.19419629, 0.61326226, 0.73849835, 0.09527054,\n",
       "        0.05181222, 0.35921023, 0.46411481, 0.4528423 , 0.87429524,\n",
       "        0.66116452, 0.62626321, 0.16446401],\n",
       "       [0.84796968, 0.9016031 , 0.78653231, 0.95624007, 0.74465037,\n",
       "        0.39024104, 0.88503049, 0.47902553, 0.61990361, 0.9021836 ,\n",
       "        0.42017842, 0.01902945, 0.01939728, 0.79114401, 0.94087209,\n",
       "        0.44201642, 0.60611512, 0.10530164, 0.06838155, 0.59785704,\n",
       "        0.98223581, 0.02715157, 0.47074424, 0.97628882, 0.24133106,\n",
       "        0.62899773, 0.41586371, 0.0475618 ],\n",
       "       [0.89264178, 0.0475634 , 0.56590951, 0.17137039, 0.43300087,\n",
       "        0.52449858, 0.17659399, 0.21568694, 0.48428192, 0.27037867,\n",
       "        0.63771718, 0.77233884, 0.44751539, 0.03188706, 0.67840028,\n",
       "        0.89486775, 0.60410814, 0.53757843, 0.65265581, 0.49339684,\n",
       "        0.54448134, 0.80567657, 0.18204974, 0.98259785, 0.74686451,\n",
       "        0.97347276, 0.09949882, 0.5496933 ],\n",
       "       [0.32849507, 0.21301569, 0.94557723, 0.50128232, 0.02085626,\n",
       "        0.53376281, 0.55866861, 0.06114378, 0.07385684, 0.9180723 ,\n",
       "        0.76720873, 0.00647311, 0.32948827, 0.69754722, 0.01394625,\n",
       "        0.48793057, 0.38348712, 0.98558546, 0.64491312, 0.35326844,\n",
       "        0.53839511, 0.02652622, 0.11505795, 0.85166397, 0.89444474,\n",
       "        0.6761529 , 0.33223778, 0.03921093],\n",
       "       [0.83806528, 0.11928232, 0.25677191, 0.39304285, 0.74112004,\n",
       "        0.85611688, 0.696962  , 0.13790257, 0.05879609, 0.43444914,\n",
       "        0.56935634, 0.23091095, 0.49205625, 0.68436421, 0.71195618,\n",
       "        0.0180918 , 0.37286789, 0.10730634, 0.29518955, 0.03302552,\n",
       "        0.2108934 , 0.55939356, 0.62454289, 0.38078841, 0.16844591,\n",
       "        0.6544661 , 0.31794669, 0.89473399],\n",
       "       [0.41778679, 0.728938  , 0.03689843, 0.22008545, 0.03354224,\n",
       "        0.67462968, 0.02156653, 0.8924847 , 0.35391685, 0.1843283 ,\n",
       "        0.51967874, 0.86119505, 0.21968346, 0.10315139, 0.59215365,\n",
       "        0.94322825, 0.56375491, 0.92199655, 0.03957142, 0.21772561,\n",
       "        0.42317262, 0.55540854, 0.99971415, 0.84141159, 0.33570627,\n",
       "        0.85415145, 0.25849309, 0.43297845],\n",
       "       [0.73187939, 0.55916117, 0.64252773, 0.66944713, 0.53913923,\n",
       "        0.88547319, 0.97180304, 0.74612632, 0.76033039, 0.18557767,\n",
       "        0.3625262 , 0.28095085, 0.94154513, 0.71203857, 0.49830525,\n",
       "        0.93178988, 0.08595115, 0.57599527, 0.62481994, 0.91599569,\n",
       "        0.64850467, 0.91647459, 0.72602153, 0.0957926 , 0.40534741,\n",
       "        0.99709961, 0.66059509, 0.90528543],\n",
       "       [0.17244656, 0.48327515, 0.85908192, 0.78893452, 0.904629  ,\n",
       "        0.45951561, 0.64732264, 0.79390499, 0.20780753, 0.81580359,\n",
       "        0.06972536, 0.00673096, 0.28283692, 0.28748697, 0.61276147,\n",
       "        0.90939025, 0.75521471, 0.7731255 , 0.57201574, 0.49811893,\n",
       "        0.33884198, 0.11828046, 0.68593789, 0.01738158, 0.92877635,\n",
       "        0.33780035, 0.5935337 , 0.98442272],\n",
       "       [0.68409573, 0.56420085, 0.24438702, 0.24260902, 0.89192049,\n",
       "        0.45584722, 0.81660541, 0.22326754, 0.03727059, 0.080369  ,\n",
       "        0.53474698, 0.38998617, 0.33564509, 0.66456734, 0.81481298,\n",
       "        0.07279635, 0.02852431, 0.49755864, 0.69569173, 0.76131165,\n",
       "        0.02409005, 0.60694076, 0.05165884, 0.86895671, 0.07019473,\n",
       "        0.22198347, 0.71711306, 0.90164408]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 첫 번째 데이터의 첫 채널의 공간 데이터에 접근하려면 다음과 같이 적습니다.\n",
    "\n",
    "x[0, 0] # 또는 x[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### im2col로 데이터 전개하기(p243)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "im2col은 입력 데이터를 필터링(가중치 계산)하기 좋게 전개하는(펼치는) 함수입니다. 아래 그림과 같이 3차원 입력 데이터에 im2col을 적용하면 2차원 행렬로 바뀝니다(정확히는 배치 안의 데이터 수까지 포함한 4차원 데이터를 2차원으로 변환합니다)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "im2col은 'image to column', 즉 '이미지에서 행렬로'라는 뜻입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (대략적인) im2col의 동작<br>\n",
    "![image.png](https://i.imgur.com/iLyjCZn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "im2col은 필터링하기 좋게 입력 데이터를 전개합니다. 구체적으로는 아래 그림과 같이 입력 데이터에서 필터를 적용하는 영역(3차원 블록)을 한 줄로 늘어놓습니다. 이 전개를 필터를 적용하는 모든 영역에서 수행하는 게 im2col입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 필터 적용 영역을 앞에서부터 순서대로 1줄로 펼친다.<br>\n",
    "![image.png](https://i.imgur.com/pRv42Kj.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 그림에서는 보기에 좋게끔 스트라이드를 크게 잡아 필터의 적용 영역이 겹치지 않도록 했지만, 실제 상황에서는 영역이 겹치는 경우가 대부분입니다. 필터 적용 영역이 겹치게 되면 im2col로 전개한 후의 원소 수가 원래 블록의 원소 수보다 많아집니다. 그래서 im2col을 사용해 구현하면 메모리를 더 많이 소비하는 단점이 있습니다. 하지만 컴퓨터는 큰 행렬을 묶어서 계산하는 데 탁월합니다. 예를 들어 행렬 계산 라이브러리(선형 대수 라이브러리) 등은 행렬 계산에 고도로 최적화되어 큰 행렬의 곱셈을 빠르게 계산할 수 있습니다. 그래서 문제를 행렬 계산으로 만들면 선형 대수 라이브러리를 활용해 효율을 높일 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 합성곱 연산의 필터 처리 상세 과정 : 필터를 세로로 1열로 전개하고, im2col이 전개한 데이터와 행렬 곱을 계산합니다. 마지막으로 출력 데이터를 변형(reshape)합니다.<br>\n",
    "![image.png](https://i.imgur.com/2IubbW3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* input.shape = (N, C, H, W)\n",
    "* filter.shape = (FN, C, FH, FW)\n",
    "* im2col.shape = (N\\*OH\\*OW, C\\*FH\\*FW)\n",
    "* im2col_W.shape = (C\\*FH\\*FW, FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 그림과 같이 im2col 방식으로 출력한 결과는 2차원 행렬입니다. CNN은 데이터를 4차원 배열로 저장하므로 2차원인 출력 데이터를 4차원으로 변형(reshape)합니다. 이상이 합성곱 계층의 구현 흐름입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 합성곱 계층 구현하기(p245)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "im2col 함수의 인터페이스는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "im2col(input_data, filter_h, filter_w, stride=1, pad=0)\n",
    "<br><br>\n",
    "* input_data : (데이터 수, 채널 수, 높이, 너비)의 4차원 배열로 이뤄진 입력 데이터\n",
    "* filter_h : 필터의 높이\n",
    "* filter_w : 필터의 너비\n",
    "* stride : 스트라이드\n",
    "* pad : 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n",
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "# im2col 함수를 적용한 두 경우 모두 2번째 차원의 원소는 75개입니다.\n",
    "# 이 값은 필터의 원소 수와 같죠(채널 3개, 5x5 데이터).\n",
    "\n",
    "from common.util import im2col\n",
    "\n",
    "x1 = np.random.rand(1,3,7,7) # (데이터 수, 채널 수, 높이, 너비)\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(col1.shape) # (9,75)\n",
    "# (N*OH*OW, C*FH*FW) = (1*3*3, 3*5*5) = (9, 75)\n",
    "# OH = (7+2*0-5)/1 + 1\n",
    "# OW = (7+2*0-5)/1 + 1\n",
    "\n",
    "x2 = np.random.rand(10,3,7,7) # 데이터 10개\n",
    "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
    "print(col2.shape) # (90,75)\n",
    "# (N*OH*OW, C*FH*FW) = (10*3*3, 3*5*5) = (90, 75)\n",
    "# OH = (7+2*0-5)/1 + 1\n",
    "# OW = (7+2*0-5)/1 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 합성곱 계층 구현\n",
    "\n",
    "class Convolution:\n",
    "    # 합성곱 계층은 필터(가중치), 편향, 스트라이드, 패딩을 인수로 받아 초기화합니다.\n",
    "    # 필터는 (FN, C, FH, FW)의 4차원 형상입니다.\n",
    "    # 여기서 FN은 필터 개수, C는 채널, FH는 필터 높이, FW는 필터 너비입니다.\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.w.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1+(H + 2*self.pad - FH)/self.stride)\n",
    "        out_w = int(1+(W + 2*self.pad - FW)/self.stride)\n",
    "        \n",
    "        col = im2col(x, FH, FW, self.stride, self.pad) # col.shape = (N*OH*OW, C*FH*FW)\n",
    "        col_W = self.W.reshape(FN, -1).T # 필터 전개 # col_W.shape = (C*FH*FW, FN)\n",
    "        out = np.dot(col, col_W) + self.b # out.shape = (N*OH*OW, FN)\n",
    "        \n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0,3,1,2) # out.shape = (N, FN, OH, OW)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 넘파이의 transpose 함수로 축 순서 변경하기 : 인덱스(번호)로 축의 순서를 변경한다.<br>\n",
    "![image.png](https://i.imgur.com/ORKEclK.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀링 계층 구현하기(p247)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "풀링 계층 구현도 합성곱 계층과 마찬가지로 im2col을 사용해 입력 데이터를 전개합니다. 단, 풀링의 경우엔 채널 쪽이 독립적이라는 점이 합성곱 계층 때와 다릅니다. 구체적으로는 아래 그림과 같이 풀링 적용 영역을 채널마다 독립적으로 전개합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 입력 데이터에 풀링 적용 영역을 전개(2x2 풀링의 예)<br>\n",
    "![image.png](https://i.imgur.com/sOoe3Am.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일단 이렇게 전개한 후, 전개한 행렬에서 행별 최댓값을 구하고 적절한 형상으로 성형하기만 하면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 풀링 계층 구현의 흐름 : 풀링 적용 영역에서 가장 큰 원소는 회색으로 표시<br>\n",
    "![image.png](https://i.imgur.com/PjihigF.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 풀링 구현 코드\n",
    "\n",
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H-self.pool_h)/self.stride)\n",
    "        out_w = int(1 + (W-self.pool_w)/self.stride)\n",
    "        \n",
    "        # 전개 (1)\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad) # col.shape = (N*OH*OW, C*PH*PW)\n",
    "        col = col.reshape(-1, sself.pool_h * self.pool_w) # col.shape(N*OH*OW*C, PH*PW)\n",
    "        \n",
    "        # 최댓값 (2)\n",
    "        out = np.max(col, axis=1) # out.shape = (N*OH*OW*C, 1)\n",
    "        \n",
    "        # 성형 (3)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0,3,1,2) # out.shape(N, C, OH, OW), out의 shape의 C가 바뀌지 않는 것을 미루어 볼 때, 풀링을 해도 채널의 수가 변하지 않는다는 것을 확인할 수 있다.\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "풀링 계층 구현은 다음의 세 단계로 진행합니다.\n",
    "1. 입력 데이터를 전개한다.\n",
    "2. 행별 최댓값을 구한다.\n",
    "3. 적절한 모양으로 성형한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최댓값 계산에서는 넘파이의 np.max 메서드를 사용할 수 있습니다. np.max는 인수로 축(axis)을 지정할 수 있는데, 이 인수로 지정한 축마다 최댓값을 구할 수 있습니다. 가령 np.max(x, axis=1)과 같이 쓰면 입력 x의 1번째 차원의 축마다 최댓값을 구합니다(2차원 배열, 즉 행렬이라면 axis=0은 열 방향, axis=1은 행 방향을 뜻합니다)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 구현하기(p250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 단순한 CNN의 네트워크 구성<br>\n",
    "![image.png](https://i.imgur.com/4ylRcZV.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "초기화 때 받는 인수\n",
    "* input_dim : 입력 데이터(채널 수, 높이, 너비)의 차원\n",
    "* conv_param : 합성곱 계층의 하이퍼파라미터(딕셔너리). 딕셔너리의 키는 다음과 같다.\n",
    " * filter_num : 필터 수\n",
    " * filter_size : 필터 크기\n",
    " * stride : 스트라이드\n",
    " * pad : 패딩\n",
    "* hidden_size : 은닉층(완전연결)의 뉴런 수\n",
    "* output_size : 출력층(완전연결)의 뉴런 수\n",
    "* weight_init_std : 초기화 때의 가중치 표준편차"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim=(1,28,28),\n",
    "                conv_param={'filter_num':30,'filter_size':5,\n",
    "                           'pad':0,'stride':1},\n",
    "                hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad)/filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2)) # Question\n",
    "        \n",
    "        # 가중치 매개변수를 초기화하는 부분\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0],\n",
    "                                           filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size,\n",
    "                                           hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "        \n",
    "        # CNN을 구성하는 계층들\n",
    "        self.layers = OrderedDict() # 순서가 있는 딕셔너리\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'],\n",
    "                                           self.params['b1'],\n",
    "                                           conv_params['stride'],\n",
    "                                           conv_params['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'],\n",
    "                                       self.params['b2'])\n",
    "        self.layers['Relu'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'],\n",
    "                                       self.params['b3'])\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "        # 추론을 수행하는 메서드\n",
    "        # 초기화 때 layers에 추가한 계층을 맨 앞에서부터 차례로 forward 메서드를 호출하며 그 결과를 다음 계층에 전달합니다.\n",
    "        def predict(self, x):\n",
    "            for layer in self.layers.values():\n",
    "                x = layer.forward(x)\n",
    "            return x\n",
    "        \n",
    "        # 손실 함수의 값을 구하는 메서드\n",
    "        # predict 메서드의 결과를 인수로 마지막 층의 forward 메서드를 호출합니다.\n",
    "        def loss(self, x, t): # x : 입력 데이터, t : 정답 레이블\n",
    "            y = self.predict(x)\n",
    "            return self.last_layer.forward(y, t)\n",
    "        \n",
    "        def gradient(self, x, t):\n",
    "            # 순전파\n",
    "            self.loss(x, t)\n",
    "            \n",
    "            # 역전파\n",
    "            dout = 1\n",
    "            dout = self.last_layer.backward(dout)\n",
    "            \n",
    "            layers = list(self.layers.values())\n",
    "            layers.reverse()\n",
    "            for layer in layers:\n",
    "                dout = layer.backward(dout)\n",
    "                \n",
    "            # 결과 저장\n",
    "            grads = {}\n",
    "            grads['W1'] = self.layers['Conv1'].dW\n",
    "            grads['b1'] = self.layers['Conv1'].db\n",
    "            grads['W2'] = self.layers['Affine1'].dW\n",
    "            grads['b2'] = self.layers['Affine1'].db\n",
    "            grads['W3'] = self.layers['Affine2'].dW\n",
    "            grads['b3'] = self.layers['Affine2'].db\n",
    "            \n",
    "            return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.29981899714787\n",
      "=== epoch:1, train acc:0.281, test acc:0.283 ===\n",
      "train loss:2.29847361114043\n",
      "train loss:2.2944583707617268\n",
      "train loss:2.2881498357797505\n",
      "train loss:2.2784760864824203\n",
      "train loss:2.2735700732026567\n",
      "train loss:2.2631316291436554\n",
      "train loss:2.2380072338118375\n",
      "train loss:2.1998897287263643\n",
      "train loss:2.201590151844411\n",
      "train loss:2.1734736525467113\n",
      "train loss:2.110978864387828\n",
      "train loss:2.1155105576566684\n",
      "train loss:1.9946449121938292\n",
      "train loss:1.9927453412334573\n",
      "train loss:1.9208409167256966\n",
      "train loss:1.8513657479256649\n",
      "train loss:1.7684960652858308\n",
      "train loss:1.7773433835785608\n",
      "train loss:1.6537887323797975\n",
      "train loss:1.5777529387501752\n",
      "train loss:1.4632153987458911\n",
      "train loss:1.2326969636674197\n",
      "train loss:1.4045168193388204\n",
      "train loss:1.1203920088567723\n",
      "train loss:1.0885696346635714\n",
      "train loss:1.1232148227049688\n",
      "train loss:1.013225789591642\n",
      "train loss:1.04521711161821\n",
      "train loss:0.8698858871739376\n",
      "train loss:0.8048096934198722\n",
      "train loss:0.7814963796727856\n",
      "train loss:0.8281573266775106\n",
      "train loss:0.7042504609507089\n",
      "train loss:0.6874509856533241\n",
      "train loss:0.7577724000093382\n",
      "train loss:0.6906626747731961\n",
      "train loss:0.5655230221362146\n",
      "train loss:0.8105968242970711\n",
      "train loss:0.5549219601314686\n",
      "train loss:0.6909969880377129\n",
      "train loss:0.49653088743376655\n",
      "train loss:0.4836160734207558\n",
      "train loss:0.6735977207522158\n",
      "train loss:0.5660953750705728\n",
      "train loss:0.5711251907447934\n",
      "train loss:0.4146718406924397\n",
      "train loss:0.41732119921215094\n",
      "train loss:0.4160822926051283\n",
      "train loss:0.7344875440508968\n",
      "train loss:0.4413040216144808\n",
      "=== epoch:2, train acc:0.82, test acc:0.809 ===\n",
      "train loss:0.549590761409302\n",
      "train loss:0.4143100327279764\n",
      "train loss:0.5210194013478394\n",
      "train loss:0.49364666948330643\n",
      "train loss:0.5166711293430126\n",
      "train loss:0.5049017144295713\n",
      "train loss:0.40067638767484604\n",
      "train loss:0.47790248057003404\n",
      "train loss:0.4538122286090073\n",
      "train loss:0.5893674324449141\n",
      "train loss:0.3725790035566422\n",
      "train loss:0.41870770733929236\n",
      "train loss:0.4542051040278347\n",
      "train loss:0.43445705100036297\n",
      "train loss:0.3535670098473115\n",
      "train loss:0.5137362428083866\n",
      "train loss:0.33637391133166905\n",
      "train loss:0.5404365426544492\n",
      "train loss:0.372084735877674\n",
      "train loss:0.4593727780509812\n",
      "train loss:0.31003288219392433\n",
      "train loss:0.42617922267804287\n",
      "train loss:0.5389338467976714\n",
      "train loss:0.429731679617351\n",
      "train loss:0.4153756125455\n",
      "train loss:0.6339998585721758\n",
      "train loss:0.3666039365317842\n",
      "train loss:0.35872746721317667\n",
      "train loss:0.33329732826877057\n",
      "train loss:0.2540034778260993\n",
      "train loss:0.32524224178438976\n",
      "train loss:0.40499216243617975\n",
      "train loss:0.44242624761716454\n",
      "train loss:0.3825228272174305\n",
      "train loss:0.31751774896169954\n",
      "train loss:0.4351682476135192\n",
      "train loss:0.4576787090812778\n",
      "train loss:0.30367687840737334\n",
      "train loss:0.44478899164835667\n",
      "train loss:0.3350220699243559\n",
      "train loss:0.3116190686402186\n",
      "train loss:0.26356433956611847\n",
      "train loss:0.4437575251457636\n",
      "train loss:0.38315095992133025\n",
      "train loss:0.4787188147005385\n",
      "train loss:0.43253373616367313\n",
      "train loss:0.4076596841565918\n",
      "train loss:0.3923859594689385\n",
      "train loss:0.2969866566690201\n",
      "train loss:0.406603575549034\n",
      "=== epoch:3, train acc:0.875, test acc:0.879 ===\n",
      "train loss:0.29022196908778713\n",
      "train loss:0.2891625975066859\n",
      "train loss:0.34197459823179527\n",
      "train loss:0.44216595280917587\n",
      "train loss:0.4391837966044009\n",
      "train loss:0.30272991737238214\n",
      "train loss:0.23484006104547286\n",
      "train loss:0.3207447777929783\n",
      "train loss:0.3268910530672295\n",
      "train loss:0.31376199455788106\n",
      "train loss:0.27375301087410764\n",
      "train loss:0.2671866248949905\n",
      "train loss:0.3738335546736385\n",
      "train loss:0.4086519814989723\n",
      "train loss:0.2939331168839104\n",
      "train loss:0.4265418513077762\n",
      "train loss:0.44098825983383594\n",
      "train loss:0.43500279836776456\n",
      "train loss:0.4927127196474924\n",
      "train loss:0.38798832298027364\n",
      "train loss:0.2550632681241254\n",
      "train loss:0.18663572249850488\n",
      "train loss:0.19904987621656065\n",
      "train loss:0.30704048148020596\n",
      "train loss:0.36379329239339087\n",
      "train loss:0.17416656897243393\n",
      "train loss:0.4333484886428376\n",
      "train loss:0.30315761098252464\n",
      "train loss:0.2695709659373486\n",
      "train loss:0.2994612955247937\n",
      "train loss:0.29833169515916036\n",
      "train loss:0.21242798866402712\n",
      "train loss:0.3547286421042635\n",
      "train loss:0.20483204992103665\n",
      "train loss:0.3165509855913141\n",
      "train loss:0.356039286574749\n",
      "train loss:0.33127351840896585\n",
      "train loss:0.20583955492331807\n",
      "train loss:0.46488199031636346\n",
      "train loss:0.265930654658762\n",
      "train loss:0.32174790876110465\n",
      "train loss:0.22106207841384184\n",
      "train loss:0.2166497274305671\n",
      "train loss:0.39933523216901123\n",
      "train loss:0.3092217879181451\n",
      "train loss:0.24482425268447816\n",
      "train loss:0.2640314788361013\n",
      "train loss:0.2256828808625772\n",
      "train loss:0.2295824927074598\n",
      "train loss:0.19377749200082747\n",
      "=== epoch:4, train acc:0.904, test acc:0.879 ===\n",
      "train loss:0.2214677066249917\n",
      "train loss:0.35938749204873555\n",
      "train loss:0.33485261686478096\n",
      "train loss:0.3378368323615188\n",
      "train loss:0.3257467276321781\n",
      "train loss:0.3277995915097342\n",
      "train loss:0.27881174338041614\n",
      "train loss:0.5244126262586487\n",
      "train loss:0.38016947684076413\n",
      "train loss:0.2562343361707489\n",
      "train loss:0.21200482486915695\n",
      "train loss:0.22931726732183552\n",
      "train loss:0.2485607248490931\n",
      "train loss:0.2983271245294859\n",
      "train loss:0.21327347181592649\n",
      "train loss:0.4256178581289069\n",
      "train loss:0.18054025035091625\n",
      "train loss:0.3330805155150596\n",
      "train loss:0.35947672773886885\n",
      "train loss:0.22323484798631044\n",
      "train loss:0.17969164336160864\n",
      "train loss:0.15896358540028527\n",
      "train loss:0.17238895214864008\n",
      "train loss:0.28662243208774757\n",
      "train loss:0.17409117548306707\n",
      "train loss:0.24572025735620262\n",
      "train loss:0.21915770008962732\n",
      "train loss:0.3880546907960465\n",
      "train loss:0.33039490011892686\n",
      "train loss:0.14287264481004716\n",
      "train loss:0.2584226590787918\n",
      "train loss:0.09848334956915622\n",
      "train loss:0.19541481365096253\n",
      "train loss:0.16932254286114454\n",
      "train loss:0.26541994519001494\n",
      "train loss:0.15110061492339427\n",
      "train loss:0.22120713947365972\n",
      "train loss:0.35907789183082917\n",
      "train loss:0.24657736823411036\n",
      "train loss:0.2124261963986687\n",
      "train loss:0.287049505006942\n",
      "train loss:0.3079844136109621\n",
      "train loss:0.17413966563530253\n",
      "train loss:0.16779661268570184\n",
      "train loss:0.2335306512410145\n",
      "train loss:0.20792043650739467\n",
      "train loss:0.31279585452403574\n",
      "train loss:0.17174763095090953\n",
      "train loss:0.15692554529697594\n",
      "train loss:0.251996045978835\n",
      "=== epoch:5, train acc:0.909, test acc:0.892 ===\n",
      "train loss:0.1662983722319841\n",
      "train loss:0.5213044249360679\n",
      "train loss:0.21240007489691803\n",
      "train loss:0.1860291723481981\n",
      "train loss:0.2320705068059804\n",
      "train loss:0.17481789942774437\n",
      "train loss:0.2148141578816563\n",
      "train loss:0.15529589358707813\n",
      "train loss:0.11141380180459212\n",
      "train loss:0.2631856101306571\n",
      "train loss:0.20041435682611602\n",
      "train loss:0.13318467084962804\n",
      "train loss:0.17930519278847026\n",
      "train loss:0.1250082307230215\n",
      "train loss:0.34209860109915646\n",
      "train loss:0.18245459631684582\n",
      "train loss:0.21678309281005245\n",
      "train loss:0.1996035144839167\n",
      "train loss:0.2598468963765452\n",
      "train loss:0.10511977088256641\n",
      "train loss:0.18154230159539606\n",
      "train loss:0.22329697938715065\n",
      "train loss:0.22258341524937486\n",
      "train loss:0.2816493574790973\n",
      "train loss:0.1789710689502237\n",
      "train loss:0.21313305279236464\n",
      "train loss:0.21017046565393382\n",
      "train loss:0.2859731048142495\n",
      "train loss:0.2094906729356091\n",
      "train loss:0.23474637794698366\n",
      "train loss:0.29105778813720723\n",
      "train loss:0.36822172544031706\n",
      "train loss:0.15668651341869425\n",
      "train loss:0.18801723806233983\n",
      "train loss:0.1873048944811086\n",
      "train loss:0.2959946441709096\n",
      "train loss:0.13545708218942543\n",
      "train loss:0.20823477837964852\n",
      "train loss:0.14164976536732496\n",
      "train loss:0.22963288190060588\n",
      "train loss:0.11316471196685773\n",
      "train loss:0.18221560094348926\n",
      "train loss:0.16982323184145753\n",
      "train loss:0.2210703131951056\n",
      "train loss:0.21460733252507982\n",
      "train loss:0.16004080099854157\n",
      "train loss:0.257397058102912\n",
      "train loss:0.124923023189167\n",
      "train loss:0.20303303296394834\n",
      "train loss:0.2289445043543278\n",
      "=== epoch:6, train acc:0.926, test acc:0.904 ===\n",
      "train loss:0.2279137562506123\n",
      "train loss:0.17593953096961384\n",
      "train loss:0.08474700949365527\n",
      "train loss:0.22113557407455656\n",
      "train loss:0.17908472791839616\n",
      "train loss:0.1512944132428462\n",
      "train loss:0.13580088743852683\n",
      "train loss:0.2199076225113212\n",
      "train loss:0.16238155067255156\n",
      "train loss:0.1689336650159661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.17862212501728766\n",
      "train loss:0.20786858012043616\n",
      "train loss:0.16242659383430127\n",
      "train loss:0.19110872232365345\n",
      "train loss:0.20774219773249178\n",
      "train loss:0.19423433450824631\n",
      "train loss:0.25074463008728515\n",
      "train loss:0.1473040006612901\n",
      "train loss:0.19758461072176786\n",
      "train loss:0.19325440398512317\n",
      "train loss:0.10807777066238573\n",
      "train loss:0.3538260035366946\n",
      "train loss:0.1961452994840477\n",
      "train loss:0.21994376084630027\n",
      "train loss:0.14808547652263324\n",
      "train loss:0.20315741187361017\n",
      "train loss:0.153677555390425\n",
      "train loss:0.2261821836292589\n",
      "train loss:0.18679502066692336\n",
      "train loss:0.13531033759639163\n",
      "train loss:0.20397818231608075\n",
      "train loss:0.09542270788982739\n",
      "train loss:0.10571869453583259\n",
      "train loss:0.19106647293191756\n",
      "train loss:0.1420590668580219\n",
      "train loss:0.2855890273258526\n",
      "train loss:0.20627888496636554\n",
      "train loss:0.13549943823335206\n",
      "train loss:0.20541987442487014\n",
      "train loss:0.09942825287541268\n",
      "train loss:0.08804539940620741\n",
      "train loss:0.11858474642757669\n",
      "train loss:0.13415301314967368\n",
      "train loss:0.2950557638901199\n",
      "train loss:0.16111873057057471\n",
      "train loss:0.17372858109238862\n",
      "train loss:0.12312016899573427\n",
      "train loss:0.17838826763448445\n",
      "train loss:0.3016689438773454\n",
      "train loss:0.26265227971321925\n",
      "=== epoch:7, train acc:0.951, test acc:0.92 ===\n",
      "train loss:0.12621300763279109\n",
      "train loss:0.1264222169664233\n",
      "train loss:0.27445537051207236\n",
      "train loss:0.1991238870505202\n",
      "train loss:0.24368364374148604\n",
      "train loss:0.18787458245458702\n",
      "train loss:0.18828367844237384\n",
      "train loss:0.15110310512493702\n",
      "train loss:0.1913056959280694\n",
      "train loss:0.1403851626300184\n",
      "train loss:0.14691916677976655\n",
      "train loss:0.12327046586399788\n",
      "train loss:0.166546497073493\n",
      "train loss:0.09079808379993054\n",
      "train loss:0.19529100435955765\n",
      "train loss:0.09051377972889771\n",
      "train loss:0.1502114557591604\n",
      "train loss:0.2192443049499867\n",
      "train loss:0.108010185090712\n",
      "train loss:0.1344098990212447\n",
      "train loss:0.1436060784205831\n",
      "train loss:0.2722135620392311\n",
      "train loss:0.1260857066680912\n",
      "train loss:0.11275516071298589\n",
      "train loss:0.21247137536734814\n",
      "train loss:0.16650491624394262\n",
      "train loss:0.18703763933210055\n",
      "train loss:0.1195708170748027\n",
      "train loss:0.08772708645836134\n",
      "train loss:0.20668686788015378\n",
      "train loss:0.13683265741258627\n",
      "train loss:0.15150368228988864\n",
      "train loss:0.16495144973297995\n",
      "train loss:0.23382418409528594\n",
      "train loss:0.3072241109831017\n",
      "train loss:0.11272502844805787\n",
      "train loss:0.07904496808436401\n",
      "train loss:0.1440248998754379\n",
      "train loss:0.2070249234983238\n",
      "train loss:0.16872549341501444\n",
      "train loss:0.12661124504604052\n",
      "train loss:0.10422536541827582\n",
      "train loss:0.25404239224327646\n",
      "train loss:0.13501720605274092\n",
      "train loss:0.08155477818777979\n",
      "train loss:0.1105438475509002\n",
      "train loss:0.16065454908977372\n",
      "train loss:0.1486940749621503\n",
      "train loss:0.1381879281560976\n",
      "train loss:0.11548950331217239\n",
      "=== epoch:8, train acc:0.952, test acc:0.933 ===\n",
      "train loss:0.1640365574990665\n",
      "train loss:0.12335342524784318\n",
      "train loss:0.224358816019384\n",
      "train loss:0.2351806507299491\n",
      "train loss:0.13234265600660886\n",
      "train loss:0.20206517407963812\n",
      "train loss:0.29671367699109924\n",
      "train loss:0.07008673710832602\n",
      "train loss:0.1603883670654186\n",
      "train loss:0.08480736361319963\n",
      "train loss:0.11975653274607204\n",
      "train loss:0.14737393868379553\n",
      "train loss:0.08329621655565482\n",
      "train loss:0.15426577526447724\n",
      "train loss:0.12085690937815796\n",
      "train loss:0.12521552467312386\n",
      "train loss:0.11340257120773942\n",
      "train loss:0.1079825897808373\n",
      "train loss:0.13427132486699528\n",
      "train loss:0.1235878176826677\n",
      "train loss:0.0739423971306255\n",
      "train loss:0.09898039048723307\n",
      "train loss:0.2126859018627066\n",
      "train loss:0.19551049539158202\n",
      "train loss:0.11602404525021624\n",
      "train loss:0.05886069967824187\n",
      "train loss:0.08619711926089567\n",
      "train loss:0.09909515268868746\n",
      "train loss:0.08733239442349637\n",
      "train loss:0.10903669542943695\n",
      "train loss:0.11343826037670106\n",
      "train loss:0.06489783713740092\n",
      "train loss:0.09991589373400762\n",
      "train loss:0.18019482057706124\n",
      "train loss:0.12272852164918908\n",
      "train loss:0.07808419972776522\n",
      "train loss:0.1511960013403831\n",
      "train loss:0.23462549358382415\n",
      "train loss:0.14201621335645004\n",
      "train loss:0.18805897945970032\n",
      "train loss:0.11358762743505375\n",
      "train loss:0.21322463787502696\n",
      "train loss:0.07685939259048383\n",
      "train loss:0.16546100787522222\n",
      "train loss:0.18128147270972905\n",
      "train loss:0.10225471823823064\n",
      "train loss:0.11767619844515036\n",
      "train loss:0.20572461001299946\n",
      "train loss:0.08065462218705532\n",
      "train loss:0.08922500638219275\n",
      "=== epoch:9, train acc:0.956, test acc:0.939 ===\n",
      "train loss:0.08495594617154595\n",
      "train loss:0.12960969448306708\n",
      "train loss:0.17883660437984752\n",
      "train loss:0.11056247338912836\n",
      "train loss:0.14561811304729919\n",
      "train loss:0.11650736985518098\n",
      "train loss:0.08612014952822258\n",
      "train loss:0.10882706864412549\n",
      "train loss:0.10479949663803653\n",
      "train loss:0.20134114724331956\n",
      "train loss:0.06960732172719905\n",
      "train loss:0.09845206828609086\n",
      "train loss:0.1329249978745566\n",
      "train loss:0.12342945646150329\n",
      "train loss:0.1392269316242333\n",
      "train loss:0.09483645484080475\n",
      "train loss:0.0839599378524723\n",
      "train loss:0.10660413640900858\n",
      "train loss:0.14373557015900912\n",
      "train loss:0.12448942693190224\n",
      "train loss:0.10220614597232788\n",
      "train loss:0.08450595266421691\n",
      "train loss:0.08497887202528169\n",
      "train loss:0.023430671820828128\n",
      "train loss:0.13211402783310594\n",
      "train loss:0.10134449849182618\n",
      "train loss:0.1168931240764865\n",
      "train loss:0.10286909763067381\n",
      "train loss:0.15590598854214374\n",
      "train loss:0.1340813655511192\n",
      "train loss:0.10934420667588181\n",
      "train loss:0.14472470899978845\n",
      "train loss:0.08678229372018728\n",
      "train loss:0.0768029910571993\n",
      "train loss:0.1323262743472474\n",
      "train loss:0.12194432609938591\n",
      "train loss:0.06472003628669577\n",
      "train loss:0.09452634955297608\n",
      "train loss:0.26795192079085256\n",
      "train loss:0.13624900826308814\n",
      "train loss:0.11936765587217567\n",
      "train loss:0.14277394587440187\n",
      "train loss:0.07387399039711356\n",
      "train loss:0.1530574890833438\n",
      "train loss:0.044526978902091875\n",
      "train loss:0.09471402383143294\n",
      "train loss:0.10370642669407157\n",
      "train loss:0.10398632808697332\n",
      "train loss:0.11534182555657142\n",
      "train loss:0.11220751712956444\n",
      "=== epoch:10, train acc:0.959, test acc:0.938 ===\n",
      "train loss:0.1380839205455646\n",
      "train loss:0.07543198597841543\n",
      "train loss:0.0786192729256171\n",
      "train loss:0.2973079623870406\n",
      "train loss:0.10211278822970549\n",
      "train loss:0.12972365378618111\n",
      "train loss:0.07909802774411133\n",
      "train loss:0.03389322244626211\n",
      "train loss:0.12860748442614547\n",
      "train loss:0.18922270609967254\n",
      "train loss:0.11362992182322217\n",
      "train loss:0.04189838381646905\n",
      "train loss:0.049213516926570194\n",
      "train loss:0.1753670880621279\n",
      "train loss:0.07437644071132757\n",
      "train loss:0.09307076682710491\n",
      "train loss:0.037055557730274025\n",
      "train loss:0.10871607754241033\n",
      "train loss:0.15106339375272004\n",
      "train loss:0.10043263078681726\n",
      "train loss:0.06616958248377865\n",
      "train loss:0.08988866901291546\n",
      "train loss:0.20870832304734005\n",
      "train loss:0.06953975811862118\n",
      "train loss:0.08355383191415008\n",
      "train loss:0.023695612827363117\n",
      "train loss:0.08974235009931521\n",
      "train loss:0.0962606548218241\n",
      "train loss:0.06074902978192746\n",
      "train loss:0.05491198358743705\n",
      "train loss:0.09264545645774584\n",
      "train loss:0.055365388884444144\n",
      "train loss:0.14840541194759424\n",
      "train loss:0.042724966195323545\n",
      "train loss:0.10894208491067849\n",
      "train loss:0.0658570326594594\n",
      "train loss:0.0346069829295467\n",
      "train loss:0.14352601543562346\n",
      "train loss:0.08659491599425878\n",
      "train loss:0.04807705976090178\n",
      "train loss:0.03905393700235503\n",
      "train loss:0.04887778072571327\n",
      "train loss:0.22426531290489435\n",
      "train loss:0.0832724211817368\n",
      "train loss:0.05093254036729448\n",
      "train loss:0.055644727706082794\n",
      "train loss:0.1515796829523072\n",
      "train loss:0.073303677729893\n",
      "train loss:0.06274480967159432\n",
      "train loss:0.08204015842087659\n",
      "=== epoch:11, train acc:0.966, test acc:0.941 ===\n",
      "train loss:0.03242523163222203\n",
      "train loss:0.06574476324650622\n",
      "train loss:0.1086697307727487\n",
      "train loss:0.14086076720484828\n",
      "train loss:0.0518787506813326\n",
      "train loss:0.17366224505176348\n",
      "train loss:0.05803083233856496\n",
      "train loss:0.07295261068763995\n",
      "train loss:0.05442214528530617\n",
      "train loss:0.023207242211493305\n",
      "train loss:0.0875513787337256\n",
      "train loss:0.1315833228994252\n",
      "train loss:0.030100442766793765\n",
      "train loss:0.04331218974798498\n",
      "train loss:0.08032592749758174\n",
      "train loss:0.09854955080405442\n",
      "train loss:0.05325399381494598\n",
      "train loss:0.03594797116923098\n",
      "train loss:0.055358165739670166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.09568802541241533\n",
      "train loss:0.08948438649119844\n",
      "train loss:0.09560256616055655\n",
      "train loss:0.08206720220847917\n",
      "train loss:0.07840328363994743\n",
      "train loss:0.04440553605213605\n",
      "train loss:0.059122263734427374\n",
      "train loss:0.03662871997066149\n",
      "train loss:0.048553103895257015\n",
      "train loss:0.09963476908891442\n",
      "train loss:0.11329887414057424\n",
      "train loss:0.0899764406719721\n",
      "train loss:0.04670305696913794\n",
      "train loss:0.0527385603230306\n",
      "train loss:0.034419423096105455\n",
      "train loss:0.056529868855250916\n",
      "train loss:0.15371481073106344\n",
      "train loss:0.04730725444903585\n",
      "train loss:0.08767535997857799\n",
      "train loss:0.06889617206264856\n",
      "train loss:0.13403574364318332\n",
      "train loss:0.0974431661954919\n",
      "train loss:0.0876243927062776\n",
      "train loss:0.060948353797491236\n",
      "train loss:0.07573156270589791\n",
      "train loss:0.1164954173818403\n",
      "train loss:0.08796817442779176\n",
      "train loss:0.0462107075035283\n",
      "train loss:0.0664797419114204\n",
      "train loss:0.10983420285912043\n",
      "train loss:0.13156120515319517\n",
      "=== epoch:12, train acc:0.975, test acc:0.948 ===\n",
      "train loss:0.09198425570013664\n",
      "train loss:0.17570169280573658\n",
      "train loss:0.025293454698941387\n",
      "train loss:0.08168769266380474\n",
      "train loss:0.11578561916110632\n",
      "train loss:0.04680501970894104\n",
      "train loss:0.04524360949918976\n",
      "train loss:0.06747294331571274\n",
      "train loss:0.1598747594990491\n",
      "train loss:0.02700137050096016\n",
      "train loss:0.0458832115404401\n",
      "train loss:0.10566889913531824\n",
      "train loss:0.11735372606542742\n",
      "train loss:0.10882534805037197\n",
      "train loss:0.037121553373571314\n",
      "train loss:0.09021024239566511\n",
      "train loss:0.08023953537948639\n",
      "train loss:0.03905499322235399\n",
      "train loss:0.03256874177304664\n",
      "train loss:0.02707058597382052\n",
      "train loss:0.045257669100290776\n",
      "train loss:0.06814471893286364\n",
      "train loss:0.07604366322590062\n",
      "train loss:0.048867783599085585\n",
      "train loss:0.020884175629802798\n",
      "train loss:0.07540107700886697\n",
      "train loss:0.11874816236336767\n",
      "train loss:0.07866532951948205\n",
      "train loss:0.05497877896617373\n",
      "train loss:0.09592505610370954\n",
      "train loss:0.04756274915019662\n",
      "train loss:0.08618230230679264\n",
      "train loss:0.08248232183596285\n",
      "train loss:0.09641873012449502\n",
      "train loss:0.06225933503008858\n",
      "train loss:0.09830613075954085\n",
      "train loss:0.09647411040570023\n",
      "train loss:0.06283834095884185\n",
      "train loss:0.07122833395108268\n",
      "train loss:0.0380743013836848\n",
      "train loss:0.09063296555925432\n",
      "train loss:0.09749418548508146\n",
      "train loss:0.05060787505938848\n",
      "train loss:0.05317178945606435\n",
      "train loss:0.052490565365252574\n",
      "train loss:0.06452432803547932\n",
      "train loss:0.0642876771651897\n",
      "train loss:0.11197104656414743\n",
      "train loss:0.08467573152423266\n",
      "train loss:0.05329544966801441\n",
      "=== epoch:13, train acc:0.974, test acc:0.952 ===\n",
      "train loss:0.04829494789407551\n",
      "train loss:0.051786479231052916\n",
      "train loss:0.0949893851163714\n",
      "train loss:0.04271390899747763\n",
      "train loss:0.022162468634673447\n",
      "train loss:0.0564257170689509\n",
      "train loss:0.07740333186218104\n",
      "train loss:0.08135423644651706\n",
      "train loss:0.06639915293552803\n",
      "train loss:0.043602966041624294\n",
      "train loss:0.06447576204330278\n",
      "train loss:0.06896838710886899\n",
      "train loss:0.11682564725939003\n",
      "train loss:0.09626278675745496\n",
      "train loss:0.0495202662901111\n",
      "train loss:0.04059095759501599\n",
      "train loss:0.02188764281446733\n",
      "train loss:0.04280248149307111\n",
      "train loss:0.04281859640395022\n",
      "train loss:0.03341836878810707\n",
      "train loss:0.08016851919420857\n",
      "train loss:0.05557735497367783\n",
      "train loss:0.03489747374628066\n",
      "train loss:0.09694739472760477\n",
      "train loss:0.06132521775619914\n",
      "train loss:0.08549968997774168\n",
      "train loss:0.07143266268264375\n",
      "train loss:0.02372877719330671\n",
      "train loss:0.05798145583818841\n",
      "train loss:0.029816610611068807\n",
      "train loss:0.055023557621774646\n",
      "train loss:0.10024124430467543\n",
      "train loss:0.07263837503584793\n",
      "train loss:0.04719566895724389\n",
      "train loss:0.058810608257353916\n",
      "train loss:0.05776134015166952\n",
      "train loss:0.02589701664738445\n",
      "train loss:0.0458266036437748\n",
      "train loss:0.05007314827377367\n",
      "train loss:0.05838150263535164\n",
      "train loss:0.06423658808252436\n",
      "train loss:0.11446131837284533\n",
      "train loss:0.05439150442884706\n",
      "train loss:0.03811525675086242\n",
      "train loss:0.048915237960566654\n",
      "train loss:0.06857729956191182\n",
      "train loss:0.05174076729136277\n",
      "train loss:0.09832002599879909\n",
      "train loss:0.03330157047228763\n",
      "train loss:0.03784658615221176\n",
      "=== epoch:14, train acc:0.976, test acc:0.946 ===\n",
      "train loss:0.07277535599821311\n",
      "train loss:0.09484659434324817\n",
      "train loss:0.03147174488175314\n",
      "train loss:0.050184870544312574\n",
      "train loss:0.039660813766488806\n",
      "train loss:0.03674339189996314\n",
      "train loss:0.06686295679647404\n",
      "train loss:0.11587258059724694\n",
      "train loss:0.10966485459941602\n",
      "train loss:0.08163246046318245\n",
      "train loss:0.07577627140149674\n",
      "train loss:0.1506509942197055\n",
      "train loss:0.05430587114377333\n",
      "train loss:0.04084542674987987\n",
      "train loss:0.0691924218602776\n",
      "train loss:0.02025639459801317\n",
      "train loss:0.11017998554387255\n",
      "train loss:0.05734535132996679\n",
      "train loss:0.056946971045652814\n",
      "train loss:0.04202258854050644\n",
      "train loss:0.032213346742270554\n",
      "train loss:0.0665985740338446\n",
      "train loss:0.052255330525401884\n",
      "train loss:0.03623744734416773\n",
      "train loss:0.058416385078217316\n",
      "train loss:0.05450092203188494\n",
      "train loss:0.07426431258780589\n",
      "train loss:0.05303172012196277\n",
      "train loss:0.05009988476672562\n",
      "train loss:0.07653697860456699\n",
      "train loss:0.08395512091989506\n",
      "train loss:0.10880176381474695\n",
      "train loss:0.046915098798207454\n",
      "train loss:0.04600289738475726\n",
      "train loss:0.03580901517757517\n",
      "train loss:0.036290409964476494\n",
      "train loss:0.03619628913553457\n",
      "train loss:0.035554466922943145\n",
      "train loss:0.044423155787142586\n",
      "train loss:0.023540721228575767\n",
      "train loss:0.03131842953560051\n",
      "train loss:0.03665584252058371\n",
      "train loss:0.04245402028679\n",
      "train loss:0.029523604010691328\n",
      "train loss:0.0586602830921886\n",
      "train loss:0.11084367838631758\n",
      "train loss:0.055493966283796776\n",
      "train loss:0.047068210253195894\n",
      "train loss:0.022818373402409144\n",
      "train loss:0.039769422472901494\n",
      "=== epoch:15, train acc:0.981, test acc:0.954 ===\n",
      "train loss:0.02728495682795911\n",
      "train loss:0.05905878156731526\n",
      "train loss:0.06910943951647856\n",
      "train loss:0.032805554824762315\n",
      "train loss:0.05389645946608249\n",
      "train loss:0.07107660809763881\n",
      "train loss:0.0314201810240651\n",
      "train loss:0.018953311055719294\n",
      "train loss:0.044254859771309975\n",
      "train loss:0.05431158106129197\n",
      "train loss:0.05091237557549113\n",
      "train loss:0.02906964057911261\n",
      "train loss:0.05013980431543812\n",
      "train loss:0.018768289475098773\n",
      "train loss:0.05245271663510536\n",
      "train loss:0.045486387072989985\n",
      "train loss:0.06705484822457466\n",
      "train loss:0.0313376031277114\n",
      "train loss:0.05631907905833129\n",
      "train loss:0.05054408753014072\n",
      "train loss:0.017942578965384575\n",
      "train loss:0.0315812457534961\n",
      "train loss:0.08799475182950105\n",
      "train loss:0.03921905250773634\n",
      "train loss:0.028130556972657653\n",
      "train loss:0.025629584670511917\n",
      "train loss:0.03732279962928392\n",
      "train loss:0.03116086362592347\n",
      "train loss:0.018784342171446016\n",
      "train loss:0.04419090902591835\n",
      "train loss:0.03620911744037881\n",
      "train loss:0.04425584336178547\n",
      "train loss:0.017989200984894992\n",
      "train loss:0.07163870508610529\n",
      "train loss:0.07723968212768391\n",
      "train loss:0.044491817577655\n",
      "train loss:0.04273221102128092\n",
      "train loss:0.04547566179574883\n",
      "train loss:0.04345429949503843\n",
      "train loss:0.098220113863092\n",
      "train loss:0.019859960037091985\n",
      "train loss:0.044908967160052306\n",
      "train loss:0.030140585212346364\n",
      "train loss:0.06514207099098356\n",
      "train loss:0.08646626779281481\n",
      "train loss:0.0264046765035779\n",
      "train loss:0.014411913651529664\n",
      "train loss:0.04039964714796475\n",
      "train loss:0.03977080954501857\n",
      "train loss:0.024952836794634257\n",
      "=== epoch:16, train acc:0.984, test acc:0.95 ===\n",
      "train loss:0.02884793334156566\n",
      "train loss:0.032399075490397354\n",
      "train loss:0.09346058247601242\n",
      "train loss:0.01682352726882921\n",
      "train loss:0.11540382353476913\n",
      "train loss:0.028463322584115124\n",
      "train loss:0.0230070180443532\n",
      "train loss:0.02278676180366271\n",
      "train loss:0.05354336741954929\n",
      "train loss:0.01868388590492488\n",
      "train loss:0.02940913887080124\n",
      "train loss:0.02482883950614991\n",
      "train loss:0.043968161181970375\n",
      "train loss:0.06324329877129009\n",
      "train loss:0.027128712838857184\n",
      "train loss:0.032334640271824394\n",
      "train loss:0.020374080890560635\n",
      "train loss:0.09415549755792223\n",
      "train loss:0.04640930960340808\n",
      "train loss:0.023921400571264365\n",
      "train loss:0.022011504222734134\n",
      "train loss:0.05782163944332594\n",
      "train loss:0.04127312653548368\n",
      "train loss:0.025657185871875054\n",
      "train loss:0.03056761395774818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.020616605256243776\n",
      "train loss:0.028688508549444592\n",
      "train loss:0.07888031754304216\n",
      "train loss:0.08571789025312646\n",
      "train loss:0.013305673307852577\n",
      "train loss:0.035054644772656116\n",
      "train loss:0.05250377148625099\n",
      "train loss:0.029586189774384174\n",
      "train loss:0.05102314099160322\n",
      "train loss:0.05903442035028223\n",
      "train loss:0.032471871975818016\n",
      "train loss:0.016250748294448735\n",
      "train loss:0.03105649354738303\n",
      "train loss:0.0527032241919665\n",
      "train loss:0.04673660112915245\n",
      "train loss:0.03532086334588391\n",
      "train loss:0.04764165202281346\n",
      "train loss:0.00983326286728119\n",
      "train loss:0.039753633931622956\n",
      "train loss:0.10617583488754306\n",
      "train loss:0.05645829146540395\n",
      "train loss:0.04890028034357871\n",
      "train loss:0.022445220661714824\n",
      "train loss:0.04570263510937044\n",
      "train loss:0.02714578149264405\n",
      "=== epoch:17, train acc:0.98, test acc:0.951 ===\n",
      "train loss:0.023162483397726957\n",
      "train loss:0.04610335229602383\n",
      "train loss:0.028848946335526763\n",
      "train loss:0.016131478977836546\n",
      "train loss:0.08632507375205233\n",
      "train loss:0.02278321396429815\n",
      "train loss:0.020597472970642475\n",
      "train loss:0.06032713723526996\n",
      "train loss:0.06337196442604048\n",
      "train loss:0.056602837211189846\n",
      "train loss:0.022862108513235793\n",
      "train loss:0.029410066098037032\n",
      "train loss:0.044486250319052505\n",
      "train loss:0.02702209248100212\n",
      "train loss:0.028815008946496155\n",
      "train loss:0.07431987318658859\n",
      "train loss:0.05853509407527484\n",
      "train loss:0.042143744211381395\n",
      "train loss:0.0387946543329141\n",
      "train loss:0.08870335279179313\n",
      "train loss:0.01845136223405749\n",
      "train loss:0.040982030769616304\n",
      "train loss:0.04530868751787903\n",
      "train loss:0.03542181868168429\n",
      "train loss:0.03816878777062618\n",
      "train loss:0.02691938681392506\n",
      "train loss:0.014693564724583871\n",
      "train loss:0.05787340179928936\n",
      "train loss:0.022392384552963853\n",
      "train loss:0.02133445201454776\n",
      "train loss:0.020486495421222578\n",
      "train loss:0.035168780957683554\n",
      "train loss:0.024641096814304434\n",
      "train loss:0.025269637392445574\n",
      "train loss:0.02167611735386409\n",
      "train loss:0.024221574719586917\n",
      "train loss:0.07258483862304606\n",
      "train loss:0.019957114768474542\n",
      "train loss:0.030533386351441393\n",
      "train loss:0.02656722865561767\n",
      "train loss:0.049003202016174655\n",
      "train loss:0.026416145725551107\n",
      "train loss:0.034494951479490395\n",
      "train loss:0.016291717235894603\n",
      "train loss:0.016961766078343862\n",
      "train loss:0.010966316282189819\n",
      "train loss:0.010815348237111912\n",
      "train loss:0.03132599842100389\n",
      "train loss:0.046725449591115124\n",
      "train loss:0.025600912802744256\n",
      "=== epoch:18, train acc:0.986, test acc:0.958 ===\n",
      "train loss:0.014309299757785549\n",
      "train loss:0.0138004288461211\n",
      "train loss:0.02916760702980238\n",
      "train loss:0.013044706371191074\n",
      "train loss:0.026909862939690696\n",
      "train loss:0.042491308909098906\n",
      "train loss:0.041958974645622905\n",
      "train loss:0.031841997805297576\n",
      "train loss:0.01403306620576098\n",
      "train loss:0.03971085191365173\n",
      "train loss:0.01764400845837137\n",
      "train loss:0.03218481721940427\n",
      "train loss:0.027234486053021666\n",
      "train loss:0.02495333666768645\n",
      "train loss:0.013703038556851031\n",
      "train loss:0.04464382631856308\n",
      "train loss:0.03976031457362733\n",
      "train loss:0.018609199360993083\n",
      "train loss:0.01521394024327831\n",
      "train loss:0.040235368045493755\n",
      "train loss:0.016823502237528386\n",
      "train loss:0.03497166205109281\n",
      "train loss:0.027533153852269208\n",
      "train loss:0.01374446325540757\n",
      "train loss:0.016768451061533723\n",
      "train loss:0.027817460490738348\n",
      "train loss:0.041454045800901944\n",
      "train loss:0.03878157809667426\n",
      "train loss:0.041269617716925264\n",
      "train loss:0.02462653448629263\n",
      "train loss:0.015159374386404119\n",
      "train loss:0.00855459801348818\n",
      "train loss:0.04507925452497418\n",
      "train loss:0.06790107996690374\n",
      "train loss:0.016102683355430997\n",
      "train loss:0.03037779426276367\n",
      "train loss:0.014890389189170896\n",
      "train loss:0.013340951945140034\n",
      "train loss:0.040716753072564144\n",
      "train loss:0.021399762345779543\n",
      "train loss:0.025746997714779105\n",
      "train loss:0.01893584225654174\n",
      "train loss:0.027509446491728748\n",
      "train loss:0.020791445588929605\n",
      "train loss:0.010021595663422624\n",
      "train loss:0.03669188562392173\n",
      "train loss:0.017216326440685645\n",
      "train loss:0.008639768695603452\n",
      "train loss:0.023707904593552855\n",
      "train loss:0.017389981722289657\n",
      "=== epoch:19, train acc:0.989, test acc:0.961 ===\n",
      "train loss:0.009110520040114329\n",
      "train loss:0.02285692376831672\n",
      "train loss:0.015903575328696677\n",
      "train loss:0.026423229272083962\n",
      "train loss:0.019074863502685054\n",
      "train loss:0.016256997707867593\n",
      "train loss:0.013791456473459338\n",
      "train loss:0.027251121695820064\n",
      "train loss:0.021678713711753415\n",
      "train loss:0.02245402646693726\n",
      "train loss:0.037364444881141344\n",
      "train loss:0.02555071338386794\n",
      "train loss:0.03456856580366086\n",
      "train loss:0.018026543087455243\n",
      "train loss:0.010321600762529675\n",
      "train loss:0.02868165419983329\n",
      "train loss:0.0449305136720318\n",
      "train loss:0.017169844068582133\n",
      "train loss:0.04820141741423636\n",
      "train loss:0.010910128410303201\n",
      "train loss:0.010289562016541803\n",
      "train loss:0.038793849221211485\n",
      "train loss:0.012210936727524727\n",
      "train loss:0.025414968611360233\n",
      "train loss:0.027906867981260648\n",
      "train loss:0.01626223250751066\n",
      "train loss:0.02239146999233618\n",
      "train loss:0.010377247906129912\n",
      "train loss:0.02063236246299655\n",
      "train loss:0.011859586886041542\n",
      "train loss:0.046381573538504695\n",
      "train loss:0.021192512490493948\n",
      "train loss:0.01592881522970214\n",
      "train loss:0.05383110462904721\n",
      "train loss:0.0059262184957873856\n",
      "train loss:0.012795778132684071\n",
      "train loss:0.024200153613550393\n",
      "train loss:0.057730419533409946\n",
      "train loss:0.03474787847069146\n",
      "train loss:0.032415215362904364\n",
      "train loss:0.029957626226635396\n",
      "train loss:0.012943450117959832\n",
      "train loss:0.0261314128685559\n",
      "train loss:0.012618359280469444\n",
      "train loss:0.025009417938541372\n",
      "train loss:0.027606889194539495\n",
      "train loss:0.019945387595978036\n",
      "train loss:0.012768579316081335\n",
      "train loss:0.017946376418242323\n",
      "train loss:0.008583481162785572\n",
      "=== epoch:20, train acc:0.995, test acc:0.955 ===\n",
      "train loss:0.013125016698913099\n",
      "train loss:0.010347746022919562\n",
      "train loss:0.015172129909021509\n",
      "train loss:0.01812456943526972\n",
      "train loss:0.012487806129586464\n",
      "train loss:0.014168750006189373\n",
      "train loss:0.020970464776978087\n",
      "train loss:0.014951204419465259\n",
      "train loss:0.020353256249596857\n",
      "train loss:0.05040458357229618\n",
      "train loss:0.01240728464969548\n",
      "train loss:0.011485430948782878\n",
      "train loss:0.029343844976702328\n",
      "train loss:0.04062905222465715\n",
      "train loss:0.016893408054161087\n",
      "train loss:0.022664987382366077\n",
      "train loss:0.009517646911253688\n",
      "train loss:0.012276163380477313\n",
      "train loss:0.01442131971506725\n",
      "train loss:0.02587026844523921\n",
      "train loss:0.014803185406963841\n",
      "train loss:0.011390014509016062\n",
      "train loss:0.011154062264152087\n",
      "train loss:0.01695304890984534\n",
      "train loss:0.019651815963201535\n",
      "train loss:0.011581560841187144\n",
      "train loss:0.006453438876828041\n",
      "train loss:0.012103322656601388\n",
      "train loss:0.011723982851477877\n",
      "train loss:0.04632283665973534\n",
      "train loss:0.030507515912952924\n",
      "train loss:0.020629890351700673\n",
      "train loss:0.007792937533532915\n",
      "train loss:0.013367557208049328\n",
      "train loss:0.04899577871673966\n",
      "train loss:0.02385848785393148\n",
      "train loss:0.020427435925887596\n",
      "train loss:0.010343662394756844\n",
      "train loss:0.02574085173418985\n",
      "train loss:0.018851819309560338\n",
      "train loss:0.023373591401574897\n",
      "train loss:0.02262766039543875\n",
      "train loss:0.008177209200399797\n",
      "train loss:0.014453819378547292\n",
      "train loss:0.01958787170124648\n",
      "train loss:0.027951064264480293\n",
      "train loss:0.013002652809023456\n",
      "train loss:0.00829625844148366\n",
      "train loss:0.01723604642644033\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.958\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from ch07.simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보존\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet(p257)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet은 손글씨 숫자를 인식하는 네트워크로, 1998년에 제안되었습니다. 아래 그림과 같이 합성곱 계층과 풀링 계층(정확히는 단순히 '원소를 줄이기'만 하는 서브샘플링 계층)을 반복하고, 마지막으로 완전연결 계층을 거치면서 결과를 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LeNet의 구성<br>\n",
    "![image.png](https://i.imgur.com/F9GdkD6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet과 '현재의 CNN'을 비교하면 몇 가지 면에서 차이가 있습니다. 첫 번째 차이는 활성화 함수입니다. LeNet은 시그모이드 함수를 사용하는 데 반해, 현재는 주로 ReLU를 사용합니다. 또, 원래의 LeNet은 서브샘플링을 하여 중간 데이터의 크기를 줄이지만 현재는 최대 풀링이 주류입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet(p258)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet과 비교해 훨씬 최근인 2012년에 발표된 AlexNet은 딥러닝 열풍을 일으키는 데 큰 역할을 했습니다. 아래 그림에서 보듯 그 구성은 기본적으로 LeNet과 크게 다르지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* AlexNet의 구성<br>\n",
    "![image.png](https://i.imgur.com/5ZI7YjN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlexNet은 합성곱 계층과 풀링 계층을 거듭하며 마지막으로 완전연결 계층을 거쳐 결과를 출력합니다. LeNet에서 큰 구조는 바뀌지 않습니다만, AlexNet에서는 다음과 같은 변화를 주었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 활성화 함수로 ReLU를 이용한다.\n",
    "* LRN(Local Response Normalization)이라는 국소적 정규화를 실시하는 계층을 이용한다.\n",
    "* 드롭아웃을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정리(p259)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CNN은 지금까지의 완전연결 계층 네트워크에 합성곱 계층과 풀링 계층을 새로 추가한다.\n",
    "* 합성곱 계층과 풀링 계층은 im2col(이미지를 행렬로 전개하는 함수)을 이용하면 간단하고 효율적으로 구현할 수 있다.\n",
    "* CNN을 시각화해보면 계층이 깊어질수록 고급 정보가 추출되는 모습을 확인할 수 있다.\n",
    "* 대표적인 CNN에는 LeNet과 AlexNet이 있다.\n",
    "* 딥러닝의 발전에는 빅데이터와 GPU가 크게 기여했다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
